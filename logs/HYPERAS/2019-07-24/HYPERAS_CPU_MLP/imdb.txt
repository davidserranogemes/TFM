Leyendo  imdb
Executing  imdb with  Feedforward  arquitecture.

>>> Imports:
#coding=utf-8

try:
    from keras.datasets import mnist
except:
    pass

try:
    from keras.datasets import fashion_mnist
except:
    pass

try:
    from keras.datasets import cifar10
except:
    pass

try:
    from keras.datasets import imdb
except:
    pass

try:
    from keras import backend as K
except:
    pass

try:
    import numpy as np
except:
    pass

try:
    import sys
except:
    pass

try:
    import time
except:
    pass

try:
    import urllib.request
except:
    pass

try:
    import os
except:
    pass

try:
    from sklearn.model_selection import StratifiedKFold
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation, Flatten
except:
    pass

try:
    from keras.layers.convolutional import Convolution2D, MaxPooling2D
except:
    pass

try:
    from keras.optimizers import SGD, Adam
except:
    pass

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 0.5),
        'Dense': hp.choice('Dense', [256, 512, 1024]),
        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),
        'Dropout_1': hp.uniform('Dropout_1', 0, 0.5),
        'Dropout_2': hp.choice('Dropout_2', ['three', 'four']),
        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),
        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),
        'batch_size': hp.choice('batch_size', [64, 128]),
    }

>>> Data
  1: 
  2: num_words=10000
  3: skip_top= 20
  4: 
  5: np_load_old = np.load
  6: 
  7: # modify the default parameters of np.load
  8: np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)
  9: 
 10: (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words,skip_top =skip_top)
 11: np.load = np_load_old
 12: 
 13: 
 14: X_train = np.zeros((len(x_train),num_words))
 15: X_test= np.zeros((len(x_test),num_words))
 16: 
 17: for i in range(1,len(x_train)):
 18: 	X_train[i,np.unique(x_train[i])[1:]] = True
 19: for i in range(1,len(x_train)):
 20: 	X_test[i,np.unique(x_test[i])[1:]] = True
 21: 
 22: nb_classes = len(np.unique(y_train))
 23: y_train = np_utils.to_categorical(y_train, nb_classes)
 24: y_test = np_utils.to_categorical(y_test, nb_classes)		
 25: 
 26: 
 27: 
 28: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3: 	num_epoch=1*200
   4: 	num_epoch=1*1
   5: 	nb_classes = y_train.shape[1]
   6: 
   7: 	model = Sequential()
   8: 	model.add(Dense(512, input_shape=X_train.shape[1:]))
   9: 	model.add(Activation('relu'))
  10: 	model.add(Dropout(space['Dropout']))
  11: 	model.add(Dense(space['Dense']))
  12: 	model.add(Activation(space['Activation']))
  13: 	model.add(Dropout(space['Dropout_1']))
  14: 
  15: 	# If we choose 'four', add an additional fourth layer
  16: 	if space['Dropout_2'] == 'four':
  17: 		model.add(Dense(100))
  18: 
  19: 		# We can also choose between complete sets of layers
  20: 
  21: 		model.add(space['add'])
  22: 		model.add(Activation('relu'))
  23: 
  24: 	model.add(Dense(nb_classes))
  25: 	model.add(Activation('softmax'))
  26: 
  27: 	model.compile(loss='categorical_crossentropy', metrics=['accuracy'],
  28: 	 					 optimizer=space['optimizer'])
  29: 
  30: 	result = model.fit(X_train, y_train,
  31: 				batch_size=space['batch_size'],
  32: 				epochs=num_epoch,
  33: 				verbose=2,
  34: 				validation_split=0.1)
  35: 	#get the highest validation accuracy of the training epochs
  36: 	validation_acc = np.amax(result.history['val_acc']) 
  37: 	print('Best validation acc of epoch:', validation_acc)
  38: 	return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}
  39: 
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 4:49
   40960/17464789 [..............................] - ETA: 1:56
  106496/17464789 [..............................] - ETA: 1:06
  245760/17464789 [..............................] - ETA: 38s 
  458752/17464789 [..............................] - ETA: 26s
  491520/17464789 [..............................] - ETA: 28s
  909312/17464789 [>.............................] - ETA: 17s
 1032192/17464789 [>.............................] - ETA: 17s
 1114112/17464789 [>.............................] - ETA: 18s
 1499136/17464789 [=>............................] - ETA: 14s
 1695744/17464789 [=>............................] - ETA: 13s
 1867776/17464789 [==>...........................] - ETA: 13s
 2023424/17464789 [==>...........................] - ETA: 13s
 2195456/17464789 [==>...........................] - ETA: 13s
 2367488/17464789 [===>..........................] - ETA: 13s
 2547712/17464789 [===>..........................] - ETA: 12s
 2686976/17464789 [===>..........................] - ETA: 12s
 2719744/17464789 [===>..........................] - ETA: 12s
 2809856/17464789 [===>..........................] - ETA: 12s
 2908160/17464789 [===>..........................] - ETA: 12s
 3047424/17464789 [====>.........................] - ETA: 11s
 3088384/17464789 [====>.........................] - ETA: 12s
 3203072/17464789 [====>.........................] - ETA: 11s
 3276800/17464789 [====>.........................] - ETA: 11s
 3416064/17464789 [====>.........................] - ETA: 11s
 3465216/17464789 [====>.........................] - ETA: 11s
 3588096/17464789 [=====>........................] - ETA: 11s
 3661824/17464789 [=====>........................] - ETA: 11s
 3760128/17464789 [=====>........................] - ETA: 11s
 3850240/17464789 [=====>........................] - ETA: 11s
 3956736/17464789 [=====>........................] - ETA: 10s
 4038656/17464789 [=====>........................] - ETA: 10s
 4128768/17464789 [======>.......................] - ETA: 10s
 4235264/17464789 [======>.......................] - ETA: 10s
 4317184/17464789 [======>.......................] - ETA: 10s
 4423680/17464789 [======>.......................] - ETA: 10s
 4513792/17464789 [======>.......................] - ETA: 10s
 4620288/17464789 [======>.......................] - ETA: 10s
 4702208/17464789 [=======>......................] - ETA: 10s
 4825088/17464789 [=======>......................] - ETA: 10s
 4915200/17464789 [=======>......................] - ETA: 9s 
 5013504/17464789 [=======>......................] - ETA: 9s
 5103616/17464789 [=======>......................] - ETA: 9s
 5210112/17464789 [=======>......................] - ETA: 9s
 5292032/17464789 [========>.....................] - ETA: 9s
 5414912/17464789 [========>.....................] - ETA: 9s
 5505024/17464789 [========>.....................] - ETA: 9s
 5611520/17464789 [========>.....................] - ETA: 9s
 5693440/17464789 [========>.....................] - ETA: 9s
 5799936/17464789 [========>.....................] - ETA: 9s
 5890048/17464789 [=========>....................] - ETA: 8s
 6012928/17464789 [=========>....................] - ETA: 8s
 6094848/17464789 [=========>....................] - ETA: 8s
 6201344/17464789 [=========>....................] - ETA: 8s
 6291456/17464789 [=========>....................] - ETA: 8s
 6389760/17464789 [=========>....................] - ETA: 8s
 6463488/17464789 [==========>...................] - ETA: 8s
 6586368/17464789 [==========>...................] - ETA: 8s
 6619136/17464789 [==========>...................] - ETA: 8s
 6709248/17464789 [==========>...................] - ETA: 8s
 6791168/17464789 [==========>...................] - ETA: 8s
 6864896/17464789 [==========>...................] - ETA: 8s
 6963200/17464789 [==========>...................] - ETA: 7s
 7020544/17464789 [===========>..................] - ETA: 7s
 7102464/17464789 [===========>..................] - ETA: 7s
 7176192/17464789 [===========>..................] - ETA: 7s
 7241728/17464789 [===========>..................] - ETA: 7s
 7348224/17464789 [===========>..................] - ETA: 7s
 7454720/17464789 [===========>..................] - ETA: 7s
 7544832/17464789 [===========>..................] - ETA: 7s
 7643136/17464789 [============>.................] - ETA: 7s
 7749632/17464789 [============>.................] - ETA: 7s
 7856128/17464789 [============>.................] - ETA: 7s
 7938048/17464789 [============>.................] - ETA: 7s
 8044544/17464789 [============>.................] - ETA: 7s
 8151040/17464789 [=============>................] - ETA: 7s
 8257536/17464789 [=============>................] - ETA: 6s
 8339456/17464789 [=============>................] - ETA: 6s
 8445952/17464789 [=============>................] - ETA: 6s
 8536064/17464789 [=============>................] - ETA: 6s
 8634368/17464789 [=============>................] - ETA: 6s
 8724480/17464789 [=============>................] - ETA: 6s
 8757248/17464789 [==============>...............] - ETA: 6s
 8863744/17464789 [==============>...............] - ETA: 6s
 8937472/17464789 [==============>...............] - ETA: 6s
 9035776/17464789 [==============>...............] - ETA: 6s
 9109504/17464789 [==============>...............] - ETA: 6s
 9175040/17464789 [==============>...............] - ETA: 6s
 9281536/17464789 [==============>...............] - ETA: 6s
 9330688/17464789 [===============>..............] - ETA: 6s
 9453568/17464789 [===============>..............] - ETA: 5s
 9527296/17464789 [===============>..............] - ETA: 5s
 9650176/17464789 [===============>..............] - ETA: 5s
 9732096/17464789 [===============>..............] - ETA: 5s
 9854976/17464789 [===============>..............] - ETA: 5s
 9928704/17464789 [================>.............] - ETA: 5s
10051584/17464789 [================>.............] - ETA: 5s
10117120/17464789 [================>.............] - ETA: 5s
10190848/17464789 [================>.............] - ETA: 5s
10289152/17464789 [================>.............] - ETA: 5s
10346496/17464789 [================>.............] - ETA: 5s
10469376/17464789 [================>.............] - ETA: 5s
10534912/17464789 [=================>............] - ETA: 5s
10625024/17464789 [=================>............] - ETA: 5s
10723328/17464789 [=================>............] - ETA: 4s
10780672/17464789 [=================>............] - ETA: 4s
10903552/17464789 [=================>............] - ETA: 4s
10952704/17464789 [=================>............] - ETA: 4s
11075584/17464789 [==================>...........] - ETA: 4s
11141120/17464789 [==================>...........] - ETA: 4s
11198464/17464789 [==================>...........] - ETA: 4s
11321344/17464789 [==================>...........] - ETA: 4s
11386880/17464789 [==================>...........] - ETA: 4s
11509760/17464789 [==================>...........] - ETA: 4s
11583488/17464789 [==================>...........] - ETA: 4s
11665408/17464789 [===================>..........] - ETA: 4s
11788288/17464789 [===================>..........] - ETA: 4s
11837440/17464789 [===================>..........] - ETA: 4s
11960320/17464789 [===================>..........] - ETA: 4s
12034048/17464789 [===================>..........] - ETA: 3s
12156928/17464789 [===================>..........] - ETA: 3s
12238848/17464789 [====================>.........] - ETA: 3s
12312576/17464789 [====================>.........] - ETA: 3s
12451840/17464789 [====================>.........] - ETA: 3s
12500992/17464789 [====================>.........] - ETA: 3s
12607488/17464789 [====================>.........] - ETA: 3s
12697600/17464789 [====================>.........] - ETA: 3s
12763136/17464789 [====================>.........] - ETA: 3s
12902400/17464789 [=====================>........] - ETA: 3s
12992512/17464789 [=====================>........] - ETA: 3s
13131776/17464789 [=====================>........] - ETA: 3s
13197312/17464789 [=====================>........] - ETA: 3s
13320192/17464789 [=====================>........] - ETA: 2s
13410304/17464789 [======================>.......] - ETA: 2s
13492224/17464789 [======================>.......] - ETA: 2s
13615104/17464789 [======================>.......] - ETA: 2s
13688832/17464789 [======================>.......] - ETA: 2s
13828096/17464789 [======================>.......] - ETA: 2s
13893632/17464789 [======================>.......] - ETA: 2s
13983744/17464789 [=======================>......] - ETA: 2s
14123008/17464789 [=======================>......] - ETA: 2s
14172160/17464789 [=======================>......] - ETA: 2s
14327808/17464789 [=======================>......] - ETA: 2s
14401536/17464789 [=======================>......] - ETA: 2s
14467072/17464789 [=======================>......] - ETA: 2s
14622720/17464789 [========================>.....] - ETA: 1s
14696448/17464789 [========================>.....] - ETA: 1s
14868480/17464789 [========================>.....] - ETA: 1s
14942208/17464789 [========================>.....] - ETA: 1s
15056896/17464789 [========================>.....] - ETA: 1s
15179776/17464789 [=========================>....] - ETA: 1s
15269888/17464789 [=========================>....] - ETA: 1s
15425536/17464789 [=========================>....] - ETA: 1s
15499264/17464789 [=========================>....] - ETA: 1s
15581184/17464789 [=========================>....] - ETA: 1s
15736832/17464789 [==========================>...] - ETA: 1s
15826944/17464789 [==========================>...] - ETA: 1s
15998976/17464789 [==========================>...] - ETA: 1s
16072704/17464789 [==========================>...] - ETA: 0s
16154624/17464789 [==========================>...] - ETA: 0s
16334848/17464789 [===========================>..] - ETA: 0s
16416768/17464789 [===========================>..] - ETA: 0s
16572416/17464789 [===========================>..] - ETA: 0s
16662528/17464789 [===========================>..] - ETA: 0s
16769024/17464789 [===========================>..] - ETA: 0s
16941056/17464789 [============================>.] - ETA: 0s
17031168/17464789 [============================>.] - ETA: 0s
17154048/17464789 [============================>.] - ETA: 0s
17309696/17464789 [============================>.] - ETA: 0s
17408000/17464789 [============================>.] - ETA: 0s
17465344/17464789 [==============================] - 12s 1us/step
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Train on 22500 samples, validate on 2500 samples
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Epoch 1/1
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                     - 7s - loss: 0.6878 - acc: 0.5393 - val_loss: 0.6675 - val_acc: 0.7036

  0%|          | 0/10 [00:07<?, ?it/s, best loss: ?]                                                    Best validation acc of epoch:
  0%|          | 0/10 [00:07<?, ?it/s, best loss: ?]                                                    0.7035999992370605
  0%|          | 0/10 [00:07<?, ?it/s, best loss: ?] 10%|█         | 1/10 [00:07<01:04,  7.22s/it, best loss: -0.7035999992370605]                                                                              Train on 22500 samples, validate on 2500 samples
 10%|█         | 1/10 [00:07<01:04,  7.22s/it, best loss: -0.7035999992370605]                                                                              Epoch 1/1
 10%|█         | 1/10 [00:07<01:04,  7.22s/it, best loss: -0.7035999992370605]                                                                               - 17s - loss: 0.4043 - acc: 0.8276 - val_loss: 0.3074 - val_acc: 0.8856

 10%|█         | 1/10 [00:24<01:04,  7.22s/it, best loss: -0.7035999992370605]                                                                              Best validation acc of epoch:
 10%|█         | 1/10 [00:24<01:04,  7.22s/it, best loss: -0.7035999992370605]                                                                              0.8856
 10%|█         | 1/10 [00:24<01:04,  7.22s/it, best loss: -0.7035999992370605] 20%|██        | 2/10 [00:24<01:22, 10.35s/it, best loss: -0.8856]                                                                              Train on 22500 samples, validate on 2500 samples
 20%|██        | 2/10 [00:25<01:22, 10.35s/it, best loss: -0.8856]                                                                  Epoch 1/1
 20%|██        | 2/10 [00:25<01:22, 10.35s/it, best loss: -0.8856]                                                                   - 20s - loss: 0.3406 - acc: 0.8451 - val_loss: 0.2721 - val_acc: 0.8924

 20%|██        | 2/10 [00:45<01:22, 10.35s/it, best loss: -0.8856]                                                                  Best validation acc of epoch:
 20%|██        | 2/10 [00:45<01:22, 10.35s/it, best loss: -0.8856]                                                                  0.8924
 20%|██        | 2/10 [00:45<01:22, 10.35s/it, best loss: -0.8856] 30%|███       | 3/10 [00:45<01:33, 13.39s/it, best loss: -0.8924]                                                                  Train on 22500 samples, validate on 2500 samples
 30%|███       | 3/10 [00:45<01:33, 13.39s/it, best loss: -0.8924]                                                                  Epoch 1/1
 30%|███       | 3/10 [00:45<01:33, 13.39s/it, best loss: -0.8924]                                                                   - 12s - loss: 0.3512 - acc: 0.8402 - val_loss: 0.2765 - val_acc: 0.8920

 30%|███       | 3/10 [00:57<01:33, 13.39s/it, best loss: -0.8924]                                                                  Best validation acc of epoch:
 30%|███       | 3/10 [00:57<01:33, 13.39s/it, best loss: -0.8924]                                                                  0.8920000000953674
 30%|███       | 3/10 [00:57<01:33, 13.39s/it, best loss: -0.8924] 40%|████      | 4/10 [00:57<01:18, 13.00s/it, best loss: -0.8924]                                                                  Train on 22500 samples, validate on 2500 samples
 40%|████      | 4/10 [00:57<01:18, 13.00s/it, best loss: -0.8924]                                                                  Epoch 1/1
 40%|████      | 4/10 [00:57<01:18, 13.00s/it, best loss: -0.8924]                                                                   - 10s - loss: 0.4488 - acc: 0.8047 - val_loss: 0.2674 - val_acc: 0.8928

 40%|████      | 4/10 [01:08<01:18, 13.00s/it, best loss: -0.8924]                                                                  Best validation acc of epoch:
 40%|████      | 4/10 [01:08<01:18, 13.00s/it, best loss: -0.8924]                                                                  0.8928000000953674
 40%|████      | 4/10 [01:08<01:18, 13.00s/it, best loss: -0.8924] 50%|█████     | 5/10 [01:08<01:01, 12.31s/it, best loss: -0.8928000000953674]                                                                              Train on 22500 samples, validate on 2500 samples
 50%|█████     | 5/10 [01:08<01:01, 12.31s/it, best loss: -0.8928000000953674]                                                                              Epoch 1/1
 50%|█████     | 5/10 [01:08<01:01, 12.31s/it, best loss: -0.8928000000953674]                                                                               - 7s - loss: 0.7680 - acc: 0.5049 - val_loss: 0.6845 - val_acc: 0.5128

 50%|█████     | 5/10 [01:14<01:01, 12.31s/it, best loss: -0.8928000000953674]                                                                              Best validation acc of epoch:
 50%|█████     | 5/10 [01:14<01:01, 12.31s/it, best loss: -0.8928000000953674]                                                                              0.5128000000476837
 50%|█████     | 5/10 [01:14<01:01, 12.31s/it, best loss: -0.8928000000953674] 60%|██████    | 6/10 [01:14<00:42, 10.65s/it, best loss: -0.8928000000953674]                                                                              Train on 22500 samples, validate on 2500 samples
 60%|██████    | 6/10 [01:15<00:42, 10.65s/it, best loss: -0.8928000000953674]                                                                              Epoch 1/1
 60%|██████    | 6/10 [01:15<00:42, 10.65s/it, best loss: -0.8928000000953674]                                                                               - 10s - loss: 0.3569 - acc: 0.8507 - val_loss: 0.2796 - val_acc: 0.8924

 60%|██████    | 6/10 [01:24<00:42, 10.65s/it, best loss: -0.8928000000953674]                                                                              Best validation acc of epoch:
 60%|██████    | 6/10 [01:24<00:42, 10.65s/it, best loss: -0.8928000000953674]                                                                              0.8923999997138977
 60%|██████    | 6/10 [01:24<00:42, 10.65s/it, best loss: -0.8928000000953674] 70%|███████   | 7/10 [01:24<00:31, 10.45s/it, best loss: -0.8928000000953674]                                                                              Train on 22500 samples, validate on 2500 samples
 70%|███████   | 7/10 [01:25<00:31, 10.45s/it, best loss: -0.8928000000953674]                                                                              Epoch 1/1
 70%|███████   | 7/10 [01:25<00:31, 10.45s/it, best loss: -0.8928000000953674]                                                                               - 7s - loss: 0.7061 - acc: 0.5029 - val_loss: 0.6925 - val_acc: 0.4876

 70%|███████   | 7/10 [01:32<00:31, 10.45s/it, best loss: -0.8928000000953674]                                                                              Best validation acc of epoch:
 70%|███████   | 7/10 [01:32<00:31, 10.45s/it, best loss: -0.8928000000953674]                                                                              0.48759999952316285
 70%|███████   | 7/10 [01:32<00:31, 10.45s/it, best loss: -0.8928000000953674] 80%|████████  | 8/10 [01:32<00:19,  9.54s/it, best loss: -0.8928000000953674]                                                                              Train on 22500 samples, validate on 2500 samples
 80%|████████  | 8/10 [01:32<00:19,  9.54s/it, best loss: -0.8928000000953674]                                                                              Epoch 1/1
 80%|████████  | 8/10 [01:32<00:19,  9.54s/it, best loss: -0.8928000000953674]                                                                               - 10s - loss: 0.3884 - acc: 0.8216 - val_loss: 0.2585 - val_acc: 0.8960

 80%|████████  | 8/10 [01:42<00:19,  9.54s/it, best loss: -0.8928000000953674]                                                                              Best validation acc of epoch:
 80%|████████  | 8/10 [01:42<00:19,  9.54s/it, best loss: -0.8928000000953674]                                                                              0.8959999997138977
 80%|████████  | 8/10 [01:42<00:19,  9.54s/it, best loss: -0.8928000000953674] 90%|█████████ | 9/10 [01:42<00:09,  9.76s/it, best loss: -0.8959999997138977]                                                                              Train on 22500 samples, validate on 2500 samples
 90%|█████████ | 9/10 [01:43<00:09,  9.76s/it, best loss: -0.8959999997138977]                                                                              Epoch 1/1
 90%|█████████ | 9/10 [01:43<00:09,  9.76s/it, best loss: -0.8959999997138977]                                                                               - 23s - loss: 0.3321 - acc: 0.8543 - val_loss: 0.2754 - val_acc: 0.8876

 90%|█████████ | 9/10 [02:06<00:09,  9.76s/it, best loss: -0.8959999997138977]                                                                              Best validation acc of epoch:
 90%|█████████ | 9/10 [02:06<00:09,  9.76s/it, best loss: -0.8959999997138977]                                                                              0.8876
 90%|█████████ | 9/10 [02:06<00:09,  9.76s/it, best loss: -0.8959999997138977]100%|██████████| 10/10 [02:06<00:00, 13.90s/it, best loss: -0.8959999997138977]
--- 144.31870102882385 seconds ---
Evalutation of best performing model:
(25000, 10000)
(25000, 10000)

   32/25000 [..............................] - ETA: 3s
  384/25000 [..............................] - ETA: 3s
  704/25000 [..............................] - ETA: 3s
 1056/25000 [>.............................] - ETA: 3s
 1376/25000 [>.............................] - ETA: 3s
 1728/25000 [=>............................] - ETA: 3s
 2080/25000 [=>............................] - ETA: 3s
 2400/25000 [=>............................] - ETA: 3s
 2752/25000 [==>...........................] - ETA: 3s
 3104/25000 [==>...........................] - ETA: 3s
 3456/25000 [===>..........................] - ETA: 3s
 3776/25000 [===>..........................] - ETA: 3s
 4096/25000 [===>..........................] - ETA: 3s
 4416/25000 [====>.........................] - ETA: 3s
 4768/25000 [====>.........................] - ETA: 3s
 5088/25000 [=====>........................] - ETA: 3s
 5376/25000 [=====>........................] - ETA: 3s
 5664/25000 [=====>........................] - ETA: 3s
 5952/25000 [======>.......................] - ETA: 3s
 6304/25000 [======>.......................] - ETA: 3s
 6656/25000 [======>.......................] - ETA: 2s
 6976/25000 [=======>......................] - ETA: 2s
 7328/25000 [=======>......................] - ETA: 2s
 7648/25000 [========>.....................] - ETA: 2s
 8000/25000 [========>.....................] - ETA: 2s
 8352/25000 [=========>....................] - ETA: 2s
 8704/25000 [=========>....................] - ETA: 2s
 9088/25000 [=========>....................] - ETA: 2s
 9472/25000 [==========>...................] - ETA: 2s
 9856/25000 [==========>...................] - ETA: 2s
10272/25000 [===========>..................] - ETA: 2s
10656/25000 [===========>..................] - ETA: 2s
11040/25000 [============>.................] - ETA: 2s
11424/25000 [============>.................] - ETA: 2s
11808/25000 [=============>................] - ETA: 2s
12192/25000 [=============>................] - ETA: 1s
12608/25000 [==============>...............] - ETA: 1s
12992/25000 [==============>...............] - ETA: 1s
13376/25000 [===============>..............] - ETA: 1s
13792/25000 [===============>..............] - ETA: 1s
14208/25000 [================>.............] - ETA: 1s
14624/25000 [================>.............] - ETA: 1s
15008/25000 [=================>............] - ETA: 1s
15424/25000 [=================>............] - ETA: 1s
15808/25000 [=================>............] - ETA: 1s
16192/25000 [==================>...........] - ETA: 1s
16608/25000 [==================>...........] - ETA: 1s
16992/25000 [===================>..........] - ETA: 1s
17376/25000 [===================>..........] - ETA: 1s
17760/25000 [====================>.........] - ETA: 1s
18144/25000 [====================>.........] - ETA: 0s
18560/25000 [=====================>........] - ETA: 0s
18944/25000 [=====================>........] - ETA: 0s
19328/25000 [======================>.......] - ETA: 0s
19712/25000 [======================>.......] - ETA: 0s
20032/25000 [=======================>......] - ETA: 0s
20352/25000 [=======================>......] - ETA: 0s
20704/25000 [=======================>......] - ETA: 0s
21056/25000 [========================>.....] - ETA: 0s
21408/25000 [========================>.....] - ETA: 0s
21760/25000 [=========================>....] - ETA: 0s
22080/25000 [=========================>....] - ETA: 0s
22432/25000 [=========================>....] - ETA: 0s
22784/25000 [==========================>...] - ETA: 0s
23104/25000 [==========================>...] - ETA: 0s
23456/25000 [===========================>..] - ETA: 0s
23776/25000 [===========================>..] - ETA: 0s
24096/25000 [===========================>..] - ETA: 0s
24416/25000 [============================>.] - ETA: 0s
24768/25000 [============================>.] - ETA: 0s
25000/25000 [==============================] - 4s 146us/step
[0.2740422092628479, 0.88568]
Time consumed:  0.04276625302102831  hours
{'Activation': 1, 'Dense': 1, 'Dropout': 0.3128745521056903, 'Dropout_1': 0.2218773160973102, 'Dropout_2': 1, 'add': 1, 'batch_size': 1, 'optimizer': 0}
