Leyendo  fashion
Executing  fashion with  Feedforward  arquitecture.

>>> Imports:
#coding=utf-8

try:
    from keras.datasets import mnist
except:
    pass

try:
    from keras.datasets import fashion_mnist
except:
    pass

try:
    from keras.datasets import cifar10
except:
    pass

try:
    from keras.datasets import imdb
except:
    pass

try:
    from keras import backend as K
except:
    pass

try:
    import numpy as np
except:
    pass

try:
    import sys
except:
    pass

try:
    import time
except:
    pass

try:
    import urllib.request
except:
    pass

try:
    import os
except:
    pass

try:
    from sklearn.model_selection import StratifiedKFold
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation, Flatten
except:
    pass

try:
    from keras.layers.convolutional import Convolution2D, MaxPooling2D
except:
    pass

try:
    from keras.optimizers import SGD, Adam
except:
    pass

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 0.5),
        'Dense': hp.choice('Dense', [256, 512, 1024]),
        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),
        'Dropout_1': hp.uniform('Dropout_1', 0, 0.5),
        'Dropout_2': hp.choice('Dropout_2', ['three', 'four']),
        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),
        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),
        'batch_size': hp.choice('batch_size', [64, 128]),
    }

>>> Data
  1: 
  2: (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
  3: 
  4: X_train = np.squeeze(X_train.reshape((X_train.shape[0], -1)))
  5: X_test = np.squeeze(X_test.reshape((X_test.shape[0], -1)))
  6: 
  7: X_train = X_train.astype('float32')
  8: X_test = X_test.astype('float32')
  9: 
 10: X_train /= 255
 11: X_test /= 255
 12: 
 13: nb_classes = len(np.unique(y_train))
 14: y_train = np_utils.to_categorical(y_train, nb_classes)
 15: y_test = np_utils.to_categorical(y_test, nb_classes)
 16: 
 17: 
 18: 
 19: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3: 	num_epoch=1*200
   4: 	num_epoch=1*1
   5: 	nb_classes = y_train.shape[1]
   6: 
   7: 	model = Sequential()
   8: 	model.add(Dense(512, input_shape=X_train.shape[1:]))
   9: 	model.add(Activation('relu'))
  10: 	model.add(Dropout(space['Dropout']))
  11: 	model.add(Dense(space['Dense']))
  12: 	model.add(Activation(space['Activation']))
  13: 	model.add(Dropout(space['Dropout_1']))
  14: 
  15: 	# If we choose 'four', add an additional fourth layer
  16: 	if space['Dropout_2'] == 'four':
  17: 		model.add(Dense(100))
  18: 
  19: 		# We can also choose between complete sets of layers
  20: 
  21: 		model.add(space['add'])
  22: 		model.add(Activation('relu'))
  23: 
  24: 	model.add(Dense(nb_classes))
  25: 	model.add(Activation('softmax'))
  26: 
  27: 	model.compile(loss='categorical_crossentropy', metrics=['accuracy'],
  28: 	 					 optimizer=space['optimizer'])
  29: 
  30: 	result = model.fit(X_train, y_train,
  31: 				batch_size=space['batch_size'],
  32: 				epochs=num_epoch,
  33: 				verbose=2,
  34: 				validation_split=0.1)
  35: 	#get the highest validation accuracy of the training epochs
  36: 	validation_acc = np.amax(result.history['val_acc']) 
  37: 	print('Best validation acc of epoch:', validation_acc)
  38: 	return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}
  39: 
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz

 8192/29515 [=======>......................] - ETA: 0s
32768/29515 [=================================] - 0s 1us/step
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz

    8192/26421880 [..............................] - ETA: 1s
   49152/26421880 [..............................] - ETA: 46s
  163840/26421880 [..............................] - ETA: 21s
  368640/26421880 [..............................] - ETA: 13s
  385024/26421880 [..............................] - ETA: 16s
  393216/26421880 [..............................] - ETA: 19s
  802816/26421880 [..............................] - ETA: 11s
  917504/26421880 [>.............................] - ETA: 12s
 1114112/26421880 [>.............................] - ETA: 11s
 1220608/26421880 [>.............................] - ETA: 11s
 1531904/26421880 [>.............................] - ETA: 10s
 1712128/26421880 [>.............................] - ETA: 10s
 1916928/26421880 [=>............................] - ETA: 9s 
 2080768/26421880 [=>............................] - ETA: 9s
 2269184/26421880 [=>............................] - ETA: 9s
 2441216/26421880 [=>............................] - ETA: 8s
 2646016/26421880 [==>...........................] - ETA: 8s
 2801664/26421880 [==>...........................] - ETA: 8s
 3006464/26421880 [==>...........................] - ETA: 8s
 3194880/26421880 [==>...........................] - ETA: 8s
 3391488/26421880 [==>...........................] - ETA: 8s
 3571712/26421880 [===>..........................] - ETA: 8s
 3768320/26421880 [===>..........................] - ETA: 7s
 3973120/26421880 [===>..........................] - ETA: 7s
 4145152/26421880 [===>..........................] - ETA: 7s
 4317184/26421880 [===>..........................] - ETA: 7s
 4481024/26421880 [====>.........................] - ETA: 7s
 4710400/26421880 [====>.........................] - ETA: 7s
 4882432/26421880 [====>.........................] - ETA: 7s
 5062656/26421880 [====>.........................] - ETA: 7s
 5259264/26421880 [====>.........................] - ETA: 6s
 5480448/26421880 [=====>........................] - ETA: 6s
 5693440/26421880 [=====>........................] - ETA: 6s
 5890048/26421880 [=====>........................] - ETA: 6s
 6111232/26421880 [=====>........................] - ETA: 6s
 6324224/26421880 [======>.......................] - ETA: 6s
 6529024/26421880 [======>.......................] - ETA: 6s
 6725632/26421880 [======>.......................] - ETA: 6s
 6971392/26421880 [======>.......................] - ETA: 6s
 7168000/26421880 [=======>......................] - ETA: 6s
 7389184/26421880 [=======>......................] - ETA: 5s
 7626752/26421880 [=======>......................] - ETA: 5s
 7831552/26421880 [=======>......................] - ETA: 5s
 8060928/26421880 [========>.....................] - ETA: 5s
 8265728/26421880 [========>.....................] - ETA: 5s
 8462336/26421880 [========>.....................] - ETA: 5s
 8667136/26421880 [========>.....................] - ETA: 5s
 8839168/26421880 [=========>....................] - ETA: 5s
 9093120/26421880 [=========>....................] - ETA: 5s
 9306112/26421880 [=========>....................] - ETA: 5s
 9502720/26421880 [=========>....................] - ETA: 5s
 9707520/26421880 [==========>...................] - ETA: 4s
 9912320/26421880 [==========>...................] - ETA: 4s
10174464/26421880 [==========>...................] - ETA: 4s
10379264/26421880 [==========>...................] - ETA: 4s
10600448/26421880 [===========>..................] - ETA: 4s
10821632/26421880 [===========>..................] - ETA: 4s
11067392/26421880 [===========>..................] - ETA: 4s
11280384/26421880 [===========>..................] - ETA: 4s
11517952/26421880 [============>.................] - ETA: 4s
11771904/26421880 [============>.................] - ETA: 4s
12009472/26421880 [============>.................] - ETA: 4s
12255232/26421880 [============>.................] - ETA: 4s
12517376/26421880 [=============>................] - ETA: 3s
12746752/26421880 [=============>................] - ETA: 3s
13000704/26421880 [=============>................] - ETA: 3s
13254656/26421880 [==============>...............] - ETA: 3s
13475840/26421880 [==============>...............] - ETA: 3s
13713408/26421880 [==============>...............] - ETA: 3s
13950976/26421880 [==============>...............] - ETA: 3s
14221312/26421880 [===============>..............] - ETA: 3s
14434304/26421880 [===============>..............] - ETA: 3s
14688256/26421880 [===============>..............] - ETA: 3s
14925824/26421880 [===============>..............] - ETA: 3s
15155200/26421880 [================>.............] - ETA: 3s
15417344/26421880 [================>.............] - ETA: 3s
15679488/26421880 [================>.............] - ETA: 2s
15917056/26421880 [=================>............] - ETA: 2s
16039936/26421880 [=================>............] - ETA: 2s
16416768/26421880 [=================>............] - ETA: 2s
16580608/26421880 [=================>............] - ETA: 2s
16752640/26421880 [==================>...........] - ETA: 2s
16941056/26421880 [==================>...........] - ETA: 2s
17088512/26421880 [==================>...........] - ETA: 2s
17309696/26421880 [==================>...........] - ETA: 2s
17481728/26421880 [==================>...........] - ETA: 2s
17653760/26421880 [===================>..........] - ETA: 2s
17850368/26421880 [===================>..........] - ETA: 2s
18046976/26421880 [===================>..........] - ETA: 2s
18259968/26421880 [===================>..........] - ETA: 2s
18497536/26421880 [====================>.........] - ETA: 2s
18694144/26421880 [====================>.........] - ETA: 2s
18956288/26421880 [====================>.........] - ETA: 2s
19152896/26421880 [====================>.........] - ETA: 1s
19357696/26421880 [====================>.........] - ETA: 1s
19603456/26421880 [=====================>........] - ETA: 1s
19832832/26421880 [=====================>........] - ETA: 1s
20045824/26421880 [=====================>........] - ETA: 1s
20250624/26421880 [=====================>........] - ETA: 1s
20471808/26421880 [======================>.......] - ETA: 1s
20701184/26421880 [======================>.......] - ETA: 1s
20897792/26421880 [======================>.......] - ETA: 1s
21135360/26421880 [======================>.......] - ETA: 1s
21405696/26421880 [=======================>......] - ETA: 1s
21594112/26421880 [=======================>......] - ETA: 1s
21798912/26421880 [=======================>......] - ETA: 1s
21995520/26421880 [=======================>......] - ETA: 1s
22224896/26421880 [========================>.....] - ETA: 1s
22454272/26421880 [========================>.....] - ETA: 1s
22691840/26421880 [========================>.....] - ETA: 1s
22913024/26421880 [=========================>....] - ETA: 0s
23126016/26421880 [=========================>....] - ETA: 0s
23339008/26421880 [=========================>....] - ETA: 0s
23609344/26421880 [=========================>....] - ETA: 0s
23855104/26421880 [==========================>...] - ETA: 0s
24051712/26421880 [==========================>...] - ETA: 0s
24281088/26421880 [==========================>...] - ETA: 0s
24502272/26421880 [==========================>...] - ETA: 0s
24592384/26421880 [==========================>...] - ETA: 0s
24903680/26421880 [===========================>..] - ETA: 0s
25059328/26421880 [===========================>..] - ETA: 0s
25264128/26421880 [===========================>..] - ETA: 0s
25468928/26421880 [===========================>..] - ETA: 0s
25624576/26421880 [============================>.] - ETA: 0s
25845760/26421880 [============================>.] - ETA: 0s
26034176/26421880 [============================>.] - ETA: 0s
26189824/26421880 [============================>.] - ETA: 0s
26394624/26421880 [============================>.] - ETA: 0s
26427392/26421880 [==============================] - 7s 0us/step
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz

8192/5148 [===============================================] - 0s 0us/step
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz

   8192/4422102 [..............................] - ETA: 0s
  16384/4422102 [..............................] - ETA: 27s
 106496/4422102 [..............................] - ETA: 7s 
 311296/4422102 [=>............................] - ETA: 4s
 737280/4422102 [====>.........................] - ETA: 1s
 843776/4422102 [====>.........................] - ETA: 1s
1179648/4422102 [=======>......................] - ETA: 1s
1351680/4422102 [========>.....................] - ETA: 1s
1540096/4422102 [=========>....................] - ETA: 1s
1761280/4422102 [==========>...................] - ETA: 1s
1802240/4422102 [===========>..................] - ETA: 1s
2056192/4422102 [============>.................] - ETA: 0s
2187264/4422102 [=============>................] - ETA: 0s
2260992/4422102 [==============>...............] - ETA: 0s
2473984/4422102 [===============>..............] - ETA: 0s
2621440/4422102 [================>.............] - ETA: 0s
2719744/4422102 [=================>............] - ETA: 0s
2842624/4422102 [==================>...........] - ETA: 0s
2973696/4422102 [===================>..........] - ETA: 0s
3104768/4422102 [====================>.........] - ETA: 0s
3235840/4422102 [====================>.........] - ETA: 0s
3366912/4422102 [=====================>........] - ETA: 0s
3497984/4422102 [======================>.......] - ETA: 0s
3596288/4422102 [=======================>......] - ETA: 0s
3760128/4422102 [========================>.....] - ETA: 0s
3866624/4422102 [=========================>....] - ETA: 0s
3964928/4422102 [=========================>....] - ETA: 0s
4145152/4422102 [===========================>..] - ETA: 0s
4243456/4422102 [===========================>..] - ETA: 0s
4407296/4422102 [============================>.] - ETA: 0s
4423680/4422102 [==============================] - 2s 0us/step
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Train on 54000 samples, validate on 6000 samples
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Epoch 1/1
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                     - 4s - loss: 1.2331 - acc: 0.5938 - val_loss: 0.7438 - val_acc: 0.7520

  0%|          | 0/10 [00:04<?, ?it/s, best loss: ?]                                                    Best validation acc of epoch:
  0%|          | 0/10 [00:04<?, ?it/s, best loss: ?]                                                    0.7520000001589457
  0%|          | 0/10 [00:04<?, ?it/s, best loss: ?] 10%|█         | 1/10 [00:04<00:39,  4.43s/it, best loss: -0.7520000001589457]                                                                              Train on 54000 samples, validate on 6000 samples
 10%|█         | 1/10 [00:04<00:39,  4.43s/it, best loss: -0.7520000001589457]                                                                              Epoch 1/1
 10%|█         | 1/10 [00:04<00:39,  4.43s/it, best loss: -0.7520000001589457]                                                                               - 9s - loss: 0.6119 - acc: 0.7767 - val_loss: 0.4389 - val_acc: 0.8362

 10%|█         | 1/10 [00:13<00:39,  4.43s/it, best loss: -0.7520000001589457]                                                                              Best validation acc of epoch:
 10%|█         | 1/10 [00:13<00:39,  4.43s/it, best loss: -0.7520000001589457]                                                                              0.8361666666666666
 10%|█         | 1/10 [00:13<00:39,  4.43s/it, best loss: -0.7520000001589457] 20%|██        | 2/10 [00:13<00:46,  5.84s/it, best loss: -0.8361666666666666]                                                                              Train on 54000 samples, validate on 6000 samples
 20%|██        | 2/10 [00:13<00:46,  5.84s/it, best loss: -0.8361666666666666]                                                                              Epoch 1/1
 20%|██        | 2/10 [00:13<00:46,  5.84s/it, best loss: -0.8361666666666666]                                                                               - 6s - loss: 0.5725 - acc: 0.7926 - val_loss: 0.4015 - val_acc: 0.8497

 20%|██        | 2/10 [00:20<00:46,  5.84s/it, best loss: -0.8361666666666666]                                                                              Best validation acc of epoch:
 20%|██        | 2/10 [00:20<00:46,  5.84s/it, best loss: -0.8361666666666666]                                                                              0.8496666668256124
 20%|██        | 2/10 [00:20<00:46,  5.84s/it, best loss: -0.8361666666666666] 30%|███       | 3/10 [00:20<00:42,  6.07s/it, best loss: -0.8496666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 30%|███       | 3/10 [00:20<00:42,  6.07s/it, best loss: -0.8496666668256124]                                                                              Epoch 1/1
 30%|███       | 3/10 [00:20<00:42,  6.07s/it, best loss: -0.8496666668256124]                                                                               - 4s - loss: 0.5970 - acc: 0.7830 - val_loss: 0.4203 - val_acc: 0.8425

 30%|███       | 3/10 [00:24<00:42,  6.07s/it, best loss: -0.8496666668256124]                                                                              Best validation acc of epoch:
 30%|███       | 3/10 [00:24<00:42,  6.07s/it, best loss: -0.8496666668256124]                                                                              0.8424999996821085
 30%|███       | 3/10 [00:24<00:42,  6.07s/it, best loss: -0.8496666668256124] 40%|████      | 4/10 [00:24<00:33,  5.57s/it, best loss: -0.8496666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 40%|████      | 4/10 [00:25<00:33,  5.57s/it, best loss: -0.8496666668256124]                                                                              Epoch 1/1
 40%|████      | 4/10 [00:25<00:33,  5.57s/it, best loss: -0.8496666668256124]                                                                               - 6s - loss: 0.7876 - acc: 0.7221 - val_loss: 0.4371 - val_acc: 0.8378

 40%|████      | 4/10 [00:30<00:33,  5.57s/it, best loss: -0.8496666668256124]                                                                              Best validation acc of epoch:
 40%|████      | 4/10 [00:30<00:33,  5.57s/it, best loss: -0.8496666668256124]                                                                              0.8378333333333333
 40%|████      | 4/10 [00:30<00:33,  5.57s/it, best loss: -0.8496666668256124] 50%|█████     | 5/10 [00:30<00:28,  5.78s/it, best loss: -0.8496666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 50%|█████     | 5/10 [00:31<00:28,  5.78s/it, best loss: -0.8496666668256124]                                                                              Epoch 1/1
 50%|█████     | 5/10 [00:31<00:28,  5.78s/it, best loss: -0.8496666668256124]                                                                               - 3s - loss: 1.9232 - acc: 0.3185 - val_loss: 1.3551 - val_acc: 0.6833

 50%|█████     | 5/10 [00:33<00:28,  5.78s/it, best loss: -0.8496666668256124]                                                                              Best validation acc of epoch:
 50%|█████     | 5/10 [00:33<00:28,  5.78s/it, best loss: -0.8496666668256124]                                                                              0.6833333338101705
 50%|█████     | 5/10 [00:33<00:28,  5.78s/it, best loss: -0.8496666668256124] 60%|██████    | 6/10 [00:33<00:19,  4.96s/it, best loss: -0.8496666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 60%|██████    | 6/10 [00:34<00:19,  4.96s/it, best loss: -0.8496666668256124]                                                                              Epoch 1/1
 60%|██████    | 6/10 [00:34<00:19,  4.96s/it, best loss: -0.8496666668256124]                                                                               - 4s - loss: 0.5790 - acc: 0.7888 - val_loss: 0.3999 - val_acc: 0.8502

 60%|██████    | 6/10 [00:38<00:19,  4.96s/it, best loss: -0.8496666668256124]                                                                              Best validation acc of epoch:
 60%|██████    | 6/10 [00:38<00:19,  4.96s/it, best loss: -0.8496666668256124]                                                                              0.8501666663487752
 60%|██████    | 6/10 [00:38<00:19,  4.96s/it, best loss: -0.8496666668256124] 70%|███████   | 7/10 [00:38<00:14,  4.76s/it, best loss: -0.8501666663487752]                                                                              Train on 54000 samples, validate on 6000 samples
 70%|███████   | 7/10 [00:38<00:14,  4.76s/it, best loss: -0.8501666663487752]                                                                              Epoch 1/1
 70%|███████   | 7/10 [00:38<00:14,  4.76s/it, best loss: -0.8501666663487752]                                                                               - 5s - loss: 1.7604 - acc: 0.4098 - val_loss: 1.1432 - val_acc: 0.6710

 70%|███████   | 7/10 [00:43<00:14,  4.76s/it, best loss: -0.8501666663487752]                                                                              Best validation acc of epoch:
 70%|███████   | 7/10 [00:43<00:14,  4.76s/it, best loss: -0.8501666663487752]                                                                              0.6709999996821085
 70%|███████   | 7/10 [00:43<00:14,  4.76s/it, best loss: -0.8501666663487752] 80%|████████  | 8/10 [00:43<00:09,  4.92s/it, best loss: -0.8501666663487752]                                                                              Train on 54000 samples, validate on 6000 samples
 80%|████████  | 8/10 [00:43<00:09,  4.92s/it, best loss: -0.8501666663487752]                                                                              Epoch 1/1
 80%|████████  | 8/10 [00:43<00:09,  4.92s/it, best loss: -0.8501666663487752]                                                                               - 4s - loss: 0.6284 - acc: 0.7690 - val_loss: 0.4513 - val_acc: 0.8347

 80%|████████  | 8/10 [00:48<00:09,  4.92s/it, best loss: -0.8501666663487752]                                                                              Best validation acc of epoch:
 80%|████████  | 8/10 [00:48<00:09,  4.92s/it, best loss: -0.8501666663487752]                                                                              0.8346666665077209
 80%|████████  | 8/10 [00:48<00:09,  4.92s/it, best loss: -0.8501666663487752] 90%|█████████ | 9/10 [00:48<00:04,  4.92s/it, best loss: -0.8501666663487752]                                                                              Train on 54000 samples, validate on 6000 samples
 90%|█████████ | 9/10 [00:48<00:04,  4.92s/it, best loss: -0.8501666663487752]                                                                              Epoch 1/1
 90%|█████████ | 9/10 [00:48<00:04,  4.92s/it, best loss: -0.8501666663487752]                                                                               - 13s - loss: 0.5169 - acc: 0.8111 - val_loss: 0.3914 - val_acc: 0.8525

 90%|█████████ | 9/10 [01:01<00:04,  4.92s/it, best loss: -0.8501666663487752]                                                                              Best validation acc of epoch:
 90%|█████████ | 9/10 [01:01<00:04,  4.92s/it, best loss: -0.8501666663487752]                                                                              0.8525000001589457
 90%|█████████ | 9/10 [01:01<00:04,  4.92s/it, best loss: -0.8501666663487752]100%|██████████| 10/10 [01:01<00:00,  7.35s/it, best loss: -0.8525000001589457]
--- 71.76325941085815 seconds ---
Evalutation of best performing model:
(60000, 784)
(10000, 784)

   32/10000 [..............................] - ETA: 0s
 1088/10000 [==>...........................] - ETA: 0s
 2144/10000 [=====>........................] - ETA: 0s
 3232/10000 [========>.....................] - ETA: 0s
 4320/10000 [===========>..................] - ETA: 0s
 5408/10000 [===============>..............] - ETA: 0s
 6464/10000 [==================>...........] - ETA: 0s
 7488/10000 [=====================>........] - ETA: 0s
 8576/10000 [========================>.....] - ETA: 0s
 9632/10000 [===========================>..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
[0.4197436882972717, 0.8502]
Time consumed:  0.020193966031074526  hours
{'Activation': 1, 'Dense': 2, 'Dropout': 0.2071809982680866, 'Dropout_1': 0.04612987161018767, 'Dropout_2': 1, 'add': 1, 'batch_size': 0, 'optimizer': 1}
