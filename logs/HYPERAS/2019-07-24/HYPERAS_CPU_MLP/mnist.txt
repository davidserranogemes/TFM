Leyendo  mnist
Executing  mnist with  Feedforward  arquitecture.

>>> Imports:
#coding=utf-8

try:
    from keras.datasets import mnist
except:
    pass

try:
    from keras.datasets import fashion_mnist
except:
    pass

try:
    from keras.datasets import cifar10
except:
    pass

try:
    from keras.datasets import imdb
except:
    pass

try:
    from keras import backend as K
except:
    pass

try:
    import numpy as np
except:
    pass

try:
    import sys
except:
    pass

try:
    import time
except:
    pass

try:
    import urllib.request
except:
    pass

try:
    import os
except:
    pass

try:
    from sklearn.model_selection import StratifiedKFold
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation, Flatten
except:
    pass

try:
    from keras.layers.convolutional import Convolution2D, MaxPooling2D
except:
    pass

try:
    from keras.optimizers import SGD, Adam
except:
    pass

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 0.5),
        'Dense': hp.choice('Dense', [256, 512, 1024]),
        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),
        'Dropout_1': hp.uniform('Dropout_1', 0, 0.5),
        'Dropout_2': hp.choice('Dropout_2', ['three', 'four']),
        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),
        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),
        'batch_size': hp.choice('batch_size', [64, 128]),
    }

>>> Data
  1: 
  2: (X_train, y_train), (X_test, y_test) = mnist.load_data()
  3: 
  4: 
  5: X_train = np.squeeze(X_train.reshape((X_train.shape[0], -1)))
  6: X_test = np.squeeze(X_test.reshape((X_test.shape[0], -1)))
  7: 
  8: X_train = X_train.astype('float32')
  9: X_test = X_test.astype('float32')
 10: 
 11: X_train /= 255
 12: X_test /= 255
 13: 
 14: nb_classes = len(np.unique(y_train))
 15: y_train = np_utils.to_categorical(y_train, nb_classes)
 16: y_test = np_utils.to_categorical(y_test, nb_classes)
 17: 
 18: 
 19: 
 20: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3: 	num_epoch=1*200
   4: 	num_epoch=1*1
   5: 	nb_classes = y_train.shape[1]
   6: 
   7: 	model = Sequential()
   8: 	model.add(Dense(512, input_shape=X_train.shape[1:]))
   9: 	model.add(Activation('relu'))
  10: 	model.add(Dropout(space['Dropout']))
  11: 	model.add(Dense(space['Dense']))
  12: 	model.add(Activation(space['Activation']))
  13: 	model.add(Dropout(space['Dropout_1']))
  14: 
  15: 	# If we choose 'four', add an additional fourth layer
  16: 	if space['Dropout_2'] == 'four':
  17: 		model.add(Dense(100))
  18: 
  19: 		# We can also choose between complete sets of layers
  20: 
  21: 		model.add(space['add'])
  22: 		model.add(Activation('relu'))
  23: 
  24: 	model.add(Dense(nb_classes))
  25: 	model.add(Activation('softmax'))
  26: 
  27: 	model.compile(loss='categorical_crossentropy', metrics=['accuracy'],
  28: 	 					 optimizer=space['optimizer'])
  29: 
  30: 	result = model.fit(X_train, y_train,
  31: 				batch_size=space['batch_size'],
  32: 				epochs=num_epoch,
  33: 				verbose=2,
  34: 				validation_split=0.1)
  35: 	#get the highest validation accuracy of the training epochs
  36: 	validation_acc = np.amax(result.history['val_acc']) 
  37: 	print('Best validation acc of epoch:', validation_acc)
  38: 	return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}
  39: 
Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz

    8192/11490434 [..............................] - ETA: 1s
   16384/11490434 [..............................] - ETA: 1:32
   49152/11490434 [..............................] - ETA: 1:13
  131072/11490434 [..............................] - ETA: 45s 
  237568/11490434 [..............................] - ETA: 34s
  360448/11490434 [..............................] - ETA: 26s
  466944/11490434 [>.............................] - ETA: 23s
  606208/11490434 [>.............................] - ETA: 19s
  729088/11490434 [>.............................] - ETA: 18s
  868352/11490434 [=>............................] - ETA: 16s
 1007616/11490434 [=>............................] - ETA: 15s
 1163264/11490434 [==>...........................] - ETA: 14s
 1228800/11490434 [==>...........................] - ETA: 14s
 1441792/11490434 [==>...........................] - ETA: 12s
 1564672/11490434 [===>..........................] - ETA: 12s
 1662976/11490434 [===>..........................] - ETA: 12s
 1769472/11490434 [===>..........................] - ETA: 11s
 1785856/11490434 [===>..........................] - ETA: 12s
 1892352/11490434 [===>..........................] - ETA: 11s
 1908736/11490434 [===>..........................] - ETA: 11s
 1998848/11490434 [====>.........................] - ETA: 11s
 2048000/11490434 [====>.........................] - ETA: 11s
 2170880/11490434 [====>.........................] - ETA: 11s
 2260992/11490434 [====>.........................] - ETA: 10s
 2310144/11490434 [=====>........................] - ETA: 10s
 2449408/11490434 [=====>........................] - ETA: 10s
 2588672/11490434 [=====>........................] - ETA: 10s
 2727936/11490434 [======>.......................] - ETA: 10s
 2834432/11490434 [======>.......................] - ETA: 9s 
 2867200/11490434 [======>.......................] - ETA: 9s
 2957312/11490434 [======>.......................] - ETA: 9s
 3006464/11490434 [======>.......................] - ETA: 9s
 3112960/11490434 [=======>......................] - ETA: 9s
 3162112/11490434 [=======>......................] - ETA: 9s
 3268608/11490434 [=======>......................] - ETA: 8s
 3317760/11490434 [=======>......................] - ETA: 8s
 3407872/11490434 [=======>......................] - ETA: 8s
 3457024/11490434 [========>.....................] - ETA: 8s
 3563520/11490434 [========>.....................] - ETA: 8s
 3612672/11490434 [========>.....................] - ETA: 8s
 3719168/11490434 [========>.....................] - ETA: 8s
 3776512/11490434 [========>.....................] - ETA: 8s
 3874816/11490434 [=========>....................] - ETA: 7s
 3932160/11490434 [=========>....................] - ETA: 7s
 4030464/11490434 [=========>....................] - ETA: 7s
 4087808/11490434 [=========>....................] - ETA: 7s
 4194304/11490434 [=========>....................] - ETA: 7s
 4243456/11490434 [==========>...................] - ETA: 7s
 4349952/11490434 [==========>...................] - ETA: 7s
 4399104/11490434 [==========>...................] - ETA: 7s
 4448256/11490434 [==========>...................] - ETA: 7s
 4554752/11490434 [==========>...................] - ETA: 7s
 4612096/11490434 [===========>..................] - ETA: 6s
 4710400/11490434 [===========>..................] - ETA: 6s
 4767744/11490434 [===========>..................] - ETA: 6s
 4890624/11490434 [===========>..................] - ETA: 6s
 5005312/11490434 [============>.................] - ETA: 6s
 5062656/11490434 [============>.................] - ETA: 6s
 5185536/11490434 [============>.................] - ETA: 6s
 5218304/11490434 [============>.................] - ETA: 6s
 5341184/11490434 [============>.................] - ETA: 6s
 5406720/11490434 [=============>................] - ETA: 5s
 5529600/11490434 [=============>................] - ETA: 5s
 5586944/11490434 [=============>................] - ETA: 5s
 5685248/11490434 [=============>................] - ETA: 5s
 5742592/11490434 [=============>................] - ETA: 5s
 5865472/11490434 [==============>...............] - ETA: 5s
 5898240/11490434 [==============>...............] - ETA: 5s
 6037504/11490434 [==============>...............] - ETA: 5s
 6070272/11490434 [==============>...............] - ETA: 5s
 6193152/11490434 [===============>..............] - ETA: 5s
 6225920/11490434 [===============>..............] - ETA: 5s
 6348800/11490434 [===============>..............] - ETA: 4s
 6397952/11490434 [===============>..............] - ETA: 4s
 6520832/11490434 [================>.............] - ETA: 4s
 6561792/11490434 [================>.............] - ETA: 4s
 6676480/11490434 [================>.............] - ETA: 4s
 6717440/11490434 [================>.............] - ETA: 4s
 6856704/11490434 [================>.............] - ETA: 4s
 6889472/11490434 [================>.............] - ETA: 4s
 7012352/11490434 [=================>............] - ETA: 4s
 7045120/11490434 [=================>............] - ETA: 4s
 7168000/11490434 [=================>............] - ETA: 4s
 7200768/11490434 [=================>............] - ETA: 4s
 7340032/11490434 [==================>...........] - ETA: 3s
 7372800/11490434 [==================>...........] - ETA: 3s
 7528448/11490434 [==================>...........] - ETA: 3s
 7536640/11490434 [==================>...........] - ETA: 3s
 7651328/11490434 [==================>...........] - ETA: 3s
 7692288/11490434 [===================>..........] - ETA: 3s
 7831552/11490434 [===================>..........] - ETA: 3s
 7864320/11490434 [===================>..........] - ETA: 3s
 8019968/11490434 [===================>..........] - ETA: 3s
 8175616/11490434 [====================>.........] - ETA: 3s
 8347648/11490434 [====================>.........] - ETA: 2s
 8470528/11490434 [=====================>........] - ETA: 2s
 8511488/11490434 [=====================>........] - ETA: 2s
 8650752/11490434 [=====================>........] - ETA: 2s
 8683520/11490434 [=====================>........] - ETA: 2s
 8806400/11490434 [=====================>........] - ETA: 2s
 8839168/11490434 [======================>.......] - ETA: 2s
 8978432/11490434 [======================>.......] - ETA: 2s
 8994816/11490434 [======================>.......] - ETA: 2s
 9101312/11490434 [======================>.......] - ETA: 2s
 9166848/11490434 [======================>.......] - ETA: 2s
 9224192/11490434 [=======================>......] - ETA: 2s
 9347072/11490434 [=======================>......] - ETA: 1s
 9396224/11490434 [=======================>......] - ETA: 1s
 9502720/11490434 [=======================>......] - ETA: 1s
 9551872/11490434 [=======================>......] - ETA: 1s
 9674752/11490434 [========================>.....] - ETA: 1s
 9723904/11490434 [========================>.....] - ETA: 1s
 9846784/11490434 [========================>.....] - ETA: 1s
 9904128/11490434 [========================>.....] - ETA: 1s
10002432/11490434 [=========================>....] - ETA: 1s
10076160/11490434 [=========================>....] - ETA: 1s
10182656/11490434 [=========================>....] - ETA: 1s
10248192/11490434 [=========================>....] - ETA: 1s
10354688/11490434 [==========================>...] - ETA: 1s
10420224/11490434 [==========================>...] - ETA: 0s
10526720/11490434 [==========================>...] - ETA: 0s
10600448/11490434 [==========================>...] - ETA: 0s
10715136/11490434 [==========================>...] - ETA: 0s
10772480/11490434 [===========================>..] - ETA: 0s
10895360/11490434 [===========================>..] - ETA: 0s
10960896/11490434 [===========================>..] - ETA: 0s
11067392/11490434 [===========================>..] - ETA: 0s
11132928/11490434 [============================>.] - ETA: 0s
11255808/11490434 [============================>.] - ETA: 0s
11329536/11490434 [============================>.] - ETA: 0s
11436032/11490434 [============================>.] - ETA: 0s
11493376/11490434 [==============================] - 10s 1us/step
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Train on 54000 samples, validate on 6000 samples
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Epoch 1/1
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                     - 4s - loss: 1.3746 - acc: 0.6268 - val_loss: 0.5862 - val_acc: 0.8780

  0%|          | 0/10 [00:04<?, ?it/s, best loss: ?]                                                    Best validation acc of epoch:
  0%|          | 0/10 [00:04<?, ?it/s, best loss: ?]                                                    0.8779999996821085
  0%|          | 0/10 [00:04<?, ?it/s, best loss: ?] 10%|█         | 1/10 [00:04<00:39,  4.38s/it, best loss: -0.8779999996821085]                                                                              Train on 54000 samples, validate on 6000 samples
 10%|█         | 1/10 [00:04<00:39,  4.38s/it, best loss: -0.8779999996821085]                                                                              Epoch 1/1
 10%|█         | 1/10 [00:04<00:39,  4.38s/it, best loss: -0.8779999996821085]                                                                               - 9s - loss: 0.3402 - acc: 0.8933 - val_loss: 0.1205 - val_acc: 0.9637

 10%|█         | 1/10 [00:13<00:39,  4.38s/it, best loss: -0.8779999996821085]                                                                              Best validation acc of epoch:
 10%|█         | 1/10 [00:13<00:39,  4.38s/it, best loss: -0.8779999996821085]                                                                              0.9636666666666667
 10%|█         | 1/10 [00:13<00:39,  4.38s/it, best loss: -0.8779999996821085] 20%|██        | 2/10 [00:13<00:46,  5.76s/it, best loss: -0.9636666666666667]                                                                              Train on 54000 samples, validate on 6000 samples
 20%|██        | 2/10 [00:13<00:46,  5.76s/it, best loss: -0.9636666666666667]                                                                              Epoch 1/1
 20%|██        | 2/10 [00:13<00:46,  5.76s/it, best loss: -0.9636666666666667]                                                                               - 6s - loss: 0.3427 - acc: 0.8967 - val_loss: 0.1143 - val_acc: 0.9663

 20%|██        | 2/10 [00:19<00:46,  5.76s/it, best loss: -0.9636666666666667]                                                                              Best validation acc of epoch:
 20%|██        | 2/10 [00:19<00:46,  5.76s/it, best loss: -0.9636666666666667]                                                                              0.9663333333333334
 20%|██        | 2/10 [00:19<00:46,  5.76s/it, best loss: -0.9636666666666667] 30%|███       | 3/10 [00:19<00:42,  6.01s/it, best loss: -0.9663333333333334]                                                                              Train on 54000 samples, validate on 6000 samples
 30%|███       | 3/10 [00:20<00:42,  6.01s/it, best loss: -0.9663333333333334]                                                                              Epoch 1/1
 30%|███       | 3/10 [00:20<00:42,  6.01s/it, best loss: -0.9663333333333334]                                                                               - 4s - loss: 0.3949 - acc: 0.8772 - val_loss: 0.1150 - val_acc: 0.9652

 30%|███       | 3/10 [00:24<00:42,  6.01s/it, best loss: -0.9663333333333334]                                                                              Best validation acc of epoch:
 30%|███       | 3/10 [00:24<00:42,  6.01s/it, best loss: -0.9663333333333334]                                                                              0.9651666668256124
 30%|███       | 3/10 [00:24<00:42,  6.01s/it, best loss: -0.9663333333333334] 40%|████      | 4/10 [00:24<00:33,  5.51s/it, best loss: -0.9663333333333334]                                                                              Train on 54000 samples, validate on 6000 samples
 40%|████      | 4/10 [00:24<00:33,  5.51s/it, best loss: -0.9663333333333334]                                                                              Epoch 1/1
 40%|████      | 4/10 [00:24<00:33,  5.51s/it, best loss: -0.9663333333333334]                                                                               - 6s - loss: 0.5404 - acc: 0.8319 - val_loss: 0.1690 - val_acc: 0.9473

 40%|████      | 4/10 [00:30<00:33,  5.51s/it, best loss: -0.9663333333333334]                                                                              Best validation acc of epoch:
 40%|████      | 4/10 [00:30<00:33,  5.51s/it, best loss: -0.9663333333333334]                                                                              0.9473333328564961
 40%|████      | 4/10 [00:30<00:33,  5.51s/it, best loss: -0.9663333333333334] 50%|█████     | 5/10 [00:30<00:28,  5.73s/it, best loss: -0.9663333333333334]                                                                              Train on 54000 samples, validate on 6000 samples
 50%|█████     | 5/10 [00:30<00:28,  5.73s/it, best loss: -0.9663333333333334]                                                                              Epoch 1/1
 50%|█████     | 5/10 [00:30<00:28,  5.73s/it, best loss: -0.9663333333333334]                                                                               - 3s - loss: 2.1863 - acc: 0.2158 - val_loss: 1.7838 - val_acc: 0.7137

 50%|█████     | 5/10 [00:33<00:28,  5.73s/it, best loss: -0.9663333333333334]                                                                              Best validation acc of epoch:
 50%|█████     | 5/10 [00:33<00:28,  5.73s/it, best loss: -0.9663333333333334]                                                                              0.7136666665077209
 50%|█████     | 5/10 [00:33<00:28,  5.73s/it, best loss: -0.9663333333333334] 60%|██████    | 6/10 [00:33<00:19,  4.91s/it, best loss: -0.9663333333333334]                                                                              Train on 54000 samples, validate on 6000 samples
 60%|██████    | 6/10 [00:33<00:19,  4.91s/it, best loss: -0.9663333333333334]                                                                              Epoch 1/1
 60%|██████    | 6/10 [00:33<00:19,  4.91s/it, best loss: -0.9663333333333334]                                                                               - 4s - loss: 0.2599 - acc: 0.9208 - val_loss: 0.1058 - val_acc: 0.9673

 60%|██████    | 6/10 [00:37<00:19,  4.91s/it, best loss: -0.9663333333333334]                                                                              Best validation acc of epoch:
 60%|██████    | 6/10 [00:37<00:19,  4.91s/it, best loss: -0.9663333333333334]                                                                              0.9673333331743876
 60%|██████    | 6/10 [00:37<00:19,  4.91s/it, best loss: -0.9663333333333334] 70%|███████   | 7/10 [00:37<00:14,  4.70s/it, best loss: -0.9673333331743876]                                                                              Train on 54000 samples, validate on 6000 samples
 70%|███████   | 7/10 [00:38<00:14,  4.70s/it, best loss: -0.9673333331743876]                                                                              Epoch 1/1
 70%|███████   | 7/10 [00:38<00:14,  4.70s/it, best loss: -0.9673333331743876]                                                                               - 5s - loss: 2.1676 - acc: 0.2440 - val_loss: 1.8529 - val_acc: 0.5352

 70%|███████   | 7/10 [00:42<00:14,  4.70s/it, best loss: -0.9673333331743876]                                                                              Best validation acc of epoch:
 70%|███████   | 7/10 [00:42<00:14,  4.70s/it, best loss: -0.9673333331743876]                                                                              0.5351666671435038
 70%|███████   | 7/10 [00:42<00:14,  4.70s/it, best loss: -0.9673333331743876] 80%|████████  | 8/10 [00:42<00:09,  4.80s/it, best loss: -0.9673333331743876]                                                                              Train on 54000 samples, validate on 6000 samples
 80%|████████  | 8/10 [00:43<00:09,  4.80s/it, best loss: -0.9673333331743876]                                                                              Epoch 1/1
 80%|████████  | 8/10 [00:43<00:09,  4.80s/it, best loss: -0.9673333331743876]                                                                               - 4s - loss: 0.3816 - acc: 0.8793 - val_loss: 0.1228 - val_acc: 0.9632

 80%|████████  | 8/10 [00:47<00:09,  4.80s/it, best loss: -0.9673333331743876]                                                                              Best validation acc of epoch:
 80%|████████  | 8/10 [00:47<00:09,  4.80s/it, best loss: -0.9673333331743876]                                                                              0.9631666661898295
 80%|████████  | 8/10 [00:47<00:09,  4.80s/it, best loss: -0.9673333331743876] 90%|█████████ | 9/10 [00:47<00:04,  4.73s/it, best loss: -0.9673333331743876]                                                                              Train on 54000 samples, validate on 6000 samples
 90%|█████████ | 9/10 [00:47<00:04,  4.73s/it, best loss: -0.9673333331743876]                                                                              Epoch 1/1
 90%|█████████ | 9/10 [00:47<00:04,  4.73s/it, best loss: -0.9673333331743876]                                                                               - 12s - loss: 0.2809 - acc: 0.9145 - val_loss: 0.1056 - val_acc: 0.9678

 90%|█████████ | 9/10 [01:00<00:04,  4.73s/it, best loss: -0.9673333331743876]                                                                              Best validation acc of epoch:
 90%|█████████ | 9/10 [01:00<00:04,  4.73s/it, best loss: -0.9673333331743876]                                                                              0.9678333333333333
 90%|█████████ | 9/10 [01:00<00:04,  4.73s/it, best loss: -0.9673333331743876]100%|██████████| 10/10 [01:00<00:00,  7.17s/it, best loss: -0.9678333333333333]
--- 71.38229489326477 seconds ---
Evalutation of best performing model:
(60000, 784)
(10000, 784)

   32/10000 [..............................] - ETA: 0s
 1120/10000 [==>...........................] - ETA: 0s
 2240/10000 [=====>........................] - ETA: 0s
 3328/10000 [========>.....................] - ETA: 0s
 4416/10000 [============>.................] - ETA: 0s
 5504/10000 [===============>..............] - ETA: 0s
 6656/10000 [==================>...........] - ETA: 0s
 7744/10000 [======================>.......] - ETA: 0s
 8832/10000 [=========================>....] - ETA: 0s
 9888/10000 [============================>.] - ETA: 0s
10000/10000 [==============================] - 0s 46us/step
[0.12381922896951437, 0.9612]
Time consumed:  0.020056096249156528  hours
{'Activation': 1, 'Dense': 2, 'Dropout': 0.2071809982680866, 'Dropout_1': 0.04612987161018767, 'Dropout_2': 1, 'add': 1, 'batch_size': 0, 'optimizer': 1}
