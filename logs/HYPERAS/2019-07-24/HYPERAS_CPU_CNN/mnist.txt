Leyendo  mnist
Executing  mnist with  Convolutional  arquitecture.

>>> Imports:
#coding=utf-8

try:
    from keras.datasets import mnist
except:
    pass

try:
    from keras.datasets import fashion_mnist
except:
    pass

try:
    from keras.datasets import cifar10
except:
    pass

try:
    from keras.datasets import imdb
except:
    pass

try:
    from keras import backend as K
except:
    pass

try:
    import numpy as np
except:
    pass

try:
    import sys
except:
    pass

try:
    import time
except:
    pass

try:
    import urllib.request
except:
    pass

try:
    import os
except:
    pass

try:
    from sklearn.model_selection import StratifiedKFold
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation, Flatten
except:
    pass

try:
    from keras.layers.convolutional import Convolution2D, MaxPooling2D
except:
    pass

try:
    from keras.optimizers import SGD, Adam
except:
    pass

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 0.5),
        'Dropout_1': hp.uniform('Dropout_1', 0, 0.5),
        'Dropout_2': hp.choice('Dropout_2', ['two','three']),
        'Dropout_3': hp.uniform('Dropout_3', 0, 0.5),
        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),
        'batch_size': hp.choice('batch_size', [64, 128]),
    }

>>> Data
  1: 
  2: (X_train, y_train), (X_test, y_test) = mnist.load_data()
  3: 
  4: X_train = X_train.reshape(X_train.shape+(1,))
  5: X_test = X_test.reshape(X_test.shape+(1,))
  6: 
  7: nb_classes = len(np.unique(y_train))
  8: y_train = np_utils.to_categorical(y_train, nb_classes)
  9: y_test = np_utils.to_categorical(y_test, nb_classes)
 10: 
 11: 
 12: 
 13: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3: 	nb_classes = y_train.shape[1]
   4: 
   5: 	num_epoch=1*100
   6: 	num_epoch=1*1
   7: 
   8: 	model = Sequential()
   9: 
  10: 	model.add(Convolution2D(32, 3, 3, border_mode='same',
  11: 	                        input_shape=X_train.shape[1:]))
  12: 	model.add(Activation('relu'))
  13: 	model.add(Convolution2D(32, 3, 3))
  14: 	model.add(Activation('relu'))
  15: 	model.add(MaxPooling2D(pool_size=(2, 2)))
  16: 	model.add(Dropout(space['Dropout']))
  17: 
  18: 	model.add(Convolution2D(64, 3, 3, border_mode='same'))
  19: 	model.add(Activation('relu'))
  20: 	model.add(Convolution2D(64, 3, 3))
  21: 	model.add(Activation('relu'))
  22: 	model.add(MaxPooling2D(pool_size=(2, 2)))
  23: 	model.add(Dropout(space['Dropout_1']))
  24: 
  25: 	if space['Dropout_2'] == 'three':
  26: 		model.add(Convolution2D(128, 3, 3, border_mode='same'))
  27: 		model.add(Activation('relu'))
  28: 		model.add(Convolution2D(128, 3, 3))
  29: 		model.add(Activation('relu'))
  30: 		model.add(MaxPooling2D(pool_size=(2, 2)))
  31: 		model.add(Dropout(space['Dropout_3']))
  32: 
  33: 	model.add(Flatten())
  34: 	model.add(Dense(512))
  35: 	model.add(Activation('relu'))
  36: 	model.add(Dropout(0.5))
  37: 	model.add(Dense(nb_classes))
  38: 	model.add(Activation('softmax'))
  39: 
  40: 	 # let's train the model using SGD + momentum (how original).
  41: 	#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
  42: 	
  43: 	model.compile(loss='categorical_crossentropy',
  44: 	              optimizer=space['optimizer'],
  45: 	              metrics=['accuracy'])
  46: 
  47: 	result = model.fit(X_train, y_train,
  48: 	          batch_size=space['batch_size'],
  49: 	          nb_epoch=num_epoch,
  50: 	          verbose=2,
  51: 	          validation_split=0.1)
  52: 	validation_acc = np.amax(result.history['val_acc']) 
  53: 	print('Best validation acc of epoch:', validation_acc)
  54: 	return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}
  55: 
Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz

    8192/11490434 [..............................] - ETA: 2s
   24576/11490434 [..............................] - ETA: 55s
   57344/11490434 [..............................] - ETA: 36s
   73728/11490434 [..............................] - ETA: 36s
  139264/11490434 [..............................] - ETA: 33s
  278528/11490434 [..............................] - ETA: 19s
  335872/11490434 [..............................] - ETA: 19s
  417792/11490434 [>.............................] - ETA: 19s
  524288/11490434 [>.............................] - ETA: 17s
  614400/11490434 [>.............................] - ETA: 16s
  696320/11490434 [>.............................] - ETA: 16s
  786432/11490434 [=>............................] - ETA: 15s
  876544/11490434 [=>............................] - ETA: 15s
  958464/11490434 [=>............................] - ETA: 15s
 1048576/11490434 [=>............................] - ETA: 14s
 1064960/11490434 [=>............................] - ETA: 14s
 1155072/11490434 [==>...........................] - ETA: 14s
 1236992/11490434 [==>...........................] - ETA: 13s
 1253376/11490434 [==>...........................] - ETA: 14s
 1343488/11490434 [==>...........................] - ETA: 13s
 1433600/11490434 [==>...........................] - ETA: 13s
 1449984/11490434 [==>...........................] - ETA: 13s
 1515520/11490434 [==>...........................] - ETA: 13s
 1556480/11490434 [===>..........................] - ETA: 13s
 1622016/11490434 [===>..........................] - ETA: 12s
 1654784/11490434 [===>..........................] - ETA: 12s
 1728512/11490434 [===>..........................] - ETA: 12s
 1761280/11490434 [===>..........................] - ETA: 12s
 1826816/11490434 [===>..........................] - ETA: 12s
 1851392/11490434 [===>..........................] - ETA: 12s
 1916928/11490434 [====>.........................] - ETA: 12s
 1949696/11490434 [====>.........................] - ETA: 12s
 2023424/11490434 [====>.........................] - ETA: 11s
 2056192/11490434 [====>.........................] - ETA: 12s
 2129920/11490434 [====>.........................] - ETA: 11s
 2162688/11490434 [====>.........................] - ETA: 11s
 2269184/11490434 [====>.........................] - ETA: 11s
 2367488/11490434 [=====>........................] - ETA: 11s
 2473984/11490434 [=====>........................] - ETA: 11s
 2580480/11490434 [=====>........................] - ETA: 11s
 2686976/11490434 [======>.......................] - ETA: 10s
 2785280/11490434 [======>.......................] - ETA: 10s
 2891776/11490434 [======>.......................] - ETA: 10s
 2998272/11490434 [======>.......................] - ETA: 10s
 3104768/11490434 [=======>......................] - ETA: 10s
 3203072/11490434 [=======>......................] - ETA: 10s
 3309568/11490434 [=======>......................] - ETA: 9s 
 3416064/11490434 [=======>......................] - ETA: 9s
 3522560/11490434 [========>.....................] - ETA: 9s
 3620864/11490434 [========>.....................] - ETA: 9s
 3743744/11490434 [========>.....................] - ETA: 9s
 3850240/11490434 [=========>....................] - ETA: 9s
 3956736/11490434 [=========>....................] - ETA: 8s
 4079616/11490434 [=========>....................] - ETA: 8s
 4177920/11490434 [=========>....................] - ETA: 8s
 4284416/11490434 [==========>...................] - ETA: 8s
 4407296/11490434 [==========>...................] - ETA: 8s
 4513792/11490434 [==========>...................] - ETA: 8s
 4636672/11490434 [===========>..................] - ETA: 7s
 4751360/11490434 [===========>..................] - ETA: 7s
 4857856/11490434 [===========>..................] - ETA: 7s
 4980736/11490434 [============>.................] - ETA: 7s
 5103616/11490434 [============>.................] - ETA: 7s
 5210112/11490434 [============>.................] - ETA: 7s
 5332992/11490434 [============>.................] - ETA: 7s
 5455872/11490434 [=============>................] - ETA: 6s
 5570560/11490434 [=============>................] - ETA: 6s
 5693440/11490434 [=============>................] - ETA: 6s
 5734400/11490434 [=============>................] - ETA: 6s
 5849088/11490434 [==============>...............] - ETA: 6s
 5971968/11490434 [==============>...............] - ETA: 6s
 6111232/11490434 [==============>...............] - ETA: 6s
 6234112/11490434 [===============>..............] - ETA: 5s
 6356992/11490434 [===============>..............] - ETA: 5s
 6479872/11490434 [===============>..............] - ETA: 5s
 6602752/11490434 [================>.............] - ETA: 5s
 6742016/11490434 [================>.............] - ETA: 5s
 6864896/11490434 [================>.............] - ETA: 5s
 6946816/11490434 [=================>............] - ETA: 5s
 7086080/11490434 [=================>............] - ETA: 4s
 7176192/11490434 [=================>............] - ETA: 4s
 7266304/11490434 [=================>............] - ETA: 4s
 7364608/11490434 [==================>...........] - ETA: 4s
 7471104/11490434 [==================>...........] - ETA: 4s
 7577600/11490434 [==================>...........] - ETA: 4s
 7684096/11490434 [===================>..........] - ETA: 4s
 7782400/11490434 [===================>..........] - ETA: 4s
 7905280/11490434 [===================>..........] - ETA: 3s
 8011776/11490434 [===================>..........] - ETA: 3s
 8134656/11490434 [====================>.........] - ETA: 3s
 8241152/11490434 [====================>.........] - ETA: 3s
 8355840/11490434 [====================>.........] - ETA: 3s
 8478720/11490434 [=====================>........] - ETA: 3s
 8601600/11490434 [=====================>........] - ETA: 3s
 8724480/11490434 [=====================>........] - ETA: 3s
 8847360/11490434 [======================>.......] - ETA: 2s
 8970240/11490434 [======================>.......] - ETA: 2s
 9093120/11490434 [======================>.......] - ETA: 2s
 9109504/11490434 [======================>.......] - ETA: 2s
 9232384/11490434 [=======================>......] - ETA: 2s
 9371648/11490434 [=======================>......] - ETA: 2s
 9494528/11490434 [=======================>......] - ETA: 2s
 9633792/11490434 [========================>.....] - ETA: 1s
 9748480/11490434 [========================>.....] - ETA: 1s
 9773056/11490434 [========================>.....] - ETA: 1s
 9977856/11490434 [=========================>....] - ETA: 1s
10067968/11490434 [=========================>....] - ETA: 1s
10149888/11490434 [=========================>....] - ETA: 1s
10256384/11490434 [=========================>....] - ETA: 1s
10346496/11490434 [==========================>...] - ETA: 1s
10362880/11490434 [==========================>...] - ETA: 1s
10444800/11490434 [==========================>...] - ETA: 1s
10534912/11490434 [==========================>...] - ETA: 1s
10551296/11490434 [==========================>...] - ETA: 1s
10657792/11490434 [==========================>...] - ETA: 0s
10764288/11490434 [===========================>..] - ETA: 0s
10862592/11490434 [===========================>..] - ETA: 0s
10969088/11490434 [===========================>..] - ETA: 0s
11091968/11490434 [===========================>..] - ETA: 0s
11198464/11490434 [============================>.] - ETA: 0s
11304960/11490434 [============================>.] - ETA: 0s
11403264/11490434 [============================>.] - ETA: 0s
11493376/11490434 [==============================] - 12s 1us/step
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Train on 54000 samples, validate on 6000 samples
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Epoch 1/1
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                     - 64s - loss: 1.1543 - acc: 0.6018 - val_loss: 0.1670 - val_acc: 0.9532

  0%|          | 0/10 [01:04<?, ?it/s, best loss: ?]                                                    Best validation acc of epoch:
  0%|          | 0/10 [01:04<?, ?it/s, best loss: ?]                                                    0.9531666661898295
  0%|          | 0/10 [01:04<?, ?it/s, best loss: ?] 10%|█         | 1/10 [01:04<09:40, 64.50s/it, best loss: -0.9531666661898295]                                                                              Train on 54000 samples, validate on 6000 samples
 10%|█         | 1/10 [01:05<09:40, 64.50s/it, best loss: -0.9531666661898295]                                                                              Epoch 1/1
 10%|█         | 1/10 [01:05<09:40, 64.50s/it, best loss: -0.9531666661898295]                                                                               - 63s - loss: 12.4523 - acc: 0.2112 - val_loss: 0.2182 - val_acc: 0.9343

 10%|█         | 1/10 [02:08<09:40, 64.50s/it, best loss: -0.9531666661898295]                                                                              Best validation acc of epoch:
 10%|█         | 1/10 [02:08<09:40, 64.50s/it, best loss: -0.9531666661898295]                                                                              0.9343333333333333
 10%|█         | 1/10 [02:08<09:40, 64.50s/it, best loss: -0.9531666661898295] 20%|██        | 2/10 [02:08<08:34, 64.33s/it, best loss: -0.9531666661898295]                                                                              Train on 54000 samples, validate on 6000 samples
 20%|██        | 2/10 [02:09<08:34, 64.33s/it, best loss: -0.9531666661898295]                                                                              Epoch 1/1
 20%|██        | 2/10 [02:09<08:34, 64.33s/it, best loss: -0.9531666661898295]                                                                               - 69s - loss: 0.3242 - acc: 0.9017 - val_loss: 0.0477 - val_acc: 0.9868

 20%|██        | 2/10 [03:17<08:34, 64.33s/it, best loss: -0.9531666661898295]                                                                              Best validation acc of epoch:
 20%|██        | 2/10 [03:17<08:34, 64.33s/it, best loss: -0.9531666661898295]                                                                              0.9868333333333333
 20%|██        | 2/10 [03:17<08:34, 64.33s/it, best loss: -0.9531666661898295] 30%|███       | 3/10 [03:17<07:41, 65.86s/it, best loss: -0.9868333333333333]                                                                              Train on 54000 samples, validate on 6000 samples
 30%|███       | 3/10 [03:18<07:41, 65.86s/it, best loss: -0.9868333333333333]                                                                              Epoch 1/1
 30%|███       | 3/10 [03:18<07:41, 65.86s/it, best loss: -0.9868333333333333]                                                                               - 62s - loss: 1.9295 - acc: 0.7524 - val_loss: 0.0567 - val_acc: 0.9823

 30%|███       | 3/10 [04:20<07:41, 65.86s/it, best loss: -0.9868333333333333]                                                                              Best validation acc of epoch:
 30%|███       | 3/10 [04:20<07:41, 65.86s/it, best loss: -0.9868333333333333]                                                                              0.9823333330154419
 30%|███       | 3/10 [04:20<07:41, 65.86s/it, best loss: -0.9868333333333333] 40%|████      | 4/10 [04:20<06:29, 64.86s/it, best loss: -0.9868333333333333]                                                                              Train on 54000 samples, validate on 6000 samples
 40%|████      | 4/10 [04:21<06:29, 64.86s/it, best loss: -0.9868333333333333]                                                                              Epoch 1/1
 40%|████      | 4/10 [04:21<06:29, 64.86s/it, best loss: -0.9868333333333333]                                                                               - 67s - loss: 0.4011 - acc: 0.8821 - val_loss: 0.0416 - val_acc: 0.9873

 40%|████      | 4/10 [05:27<06:29, 64.86s/it, best loss: -0.9868333333333333]                                                                              Best validation acc of epoch:
 40%|████      | 4/10 [05:27<06:29, 64.86s/it, best loss: -0.9868333333333333]                                                                              0.9873333328564962
 40%|████      | 4/10 [05:27<06:29, 64.86s/it, best loss: -0.9868333333333333] 50%|█████     | 5/10 [05:27<05:28, 65.63s/it, best loss: -0.9873333328564962]                                                                              Train on 54000 samples, validate on 6000 samples
 50%|█████     | 5/10 [05:28<05:28, 65.63s/it, best loss: -0.9873333328564962]                                                                              Epoch 1/1
 50%|█████     | 5/10 [05:28<05:28, 65.63s/it, best loss: -0.9873333328564962]                                                                               - 60s - loss: 14.5013 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000

 50%|█████     | 5/10 [06:28<05:28, 65.63s/it, best loss: -0.9873333328564962]                                                                              Best validation acc of epoch:
 50%|█████     | 5/10 [06:28<05:28, 65.63s/it, best loss: -0.9873333328564962]                                                                              0.09999999996026357
 50%|█████     | 5/10 [06:28<05:28, 65.63s/it, best loss: -0.9873333328564962] 60%|██████    | 6/10 [06:28<04:16, 64.05s/it, best loss: -0.9873333328564962]                                                                              Train on 54000 samples, validate on 6000 samples
 60%|██████    | 6/10 [06:28<04:16, 64.05s/it, best loss: -0.9873333328564962]                                                                              Epoch 1/1
 60%|██████    | 6/10 [06:28<04:16, 64.05s/it, best loss: -0.9873333328564962]                                                                               - 61s - loss: 12.3976 - acc: 0.2048 - val_loss: 0.1752 - val_acc: 0.9457

 60%|██████    | 6/10 [07:29<04:16, 64.05s/it, best loss: -0.9873333328564962]                                                                              Best validation acc of epoch:
 60%|██████    | 6/10 [07:29<04:16, 64.05s/it, best loss: -0.9873333328564962]                                                                              0.9456666665077209
 60%|██████    | 6/10 [07:29<04:16, 64.05s/it, best loss: -0.9873333328564962] 70%|███████   | 7/10 [07:29<03:09, 63.29s/it, best loss: -0.9873333328564962]                                                                              Train on 54000 samples, validate on 6000 samples
 70%|███████   | 7/10 [07:30<03:09, 63.29s/it, best loss: -0.9873333328564962]                                                                              Epoch 1/1
 70%|███████   | 7/10 [07:30<03:09, 63.29s/it, best loss: -0.9873333328564962]                                                                               - 61s - loss: 0.7442 - acc: 0.8362 - val_loss: 0.0768 - val_acc: 0.9768

 70%|███████   | 7/10 [08:31<03:09, 63.29s/it, best loss: -0.9873333328564962]                                                                              Best validation acc of epoch:
 70%|███████   | 7/10 [08:31<03:09, 63.29s/it, best loss: -0.9873333328564962]                                                                              0.9768333328564962
 70%|███████   | 7/10 [08:31<03:09, 63.29s/it, best loss: -0.9873333328564962] 80%|████████  | 8/10 [08:31<02:05, 62.72s/it, best loss: -0.9873333328564962]                                                                              Train on 54000 samples, validate on 6000 samples
 80%|████████  | 8/10 [08:31<02:05, 62.72s/it, best loss: -0.9873333328564962]                                                                              Epoch 1/1
 80%|████████  | 8/10 [08:31<02:05, 62.72s/it, best loss: -0.9873333328564962]                                                                               - 67s - loss: 0.4651 - acc: 0.8597 - val_loss: 0.0468 - val_acc: 0.9872

 80%|████████  | 8/10 [09:38<02:05, 62.72s/it, best loss: -0.9873333328564962]                                                                              Best validation acc of epoch:
 80%|████████  | 8/10 [09:38<02:05, 62.72s/it, best loss: -0.9873333328564962]                                                                              0.9871666661898295
 80%|████████  | 8/10 [09:38<02:05, 62.72s/it, best loss: -0.9873333328564962] 90%|█████████ | 9/10 [09:38<01:04, 64.20s/it, best loss: -0.9873333328564962]                                                                              Train on 54000 samples, validate on 6000 samples
 90%|█████████ | 9/10 [09:39<01:04, 64.20s/it, best loss: -0.9873333328564962]                                                                              Epoch 1/1
 90%|█████████ | 9/10 [09:39<01:04, 64.20s/it, best loss: -0.9873333328564962]                                                                               - 70s - loss: 0.3435 - acc: 0.8942 - val_loss: 0.0519 - val_acc: 0.9853

 90%|█████████ | 9/10 [10:49<01:04, 64.20s/it, best loss: -0.9873333328564962]                                                                              Best validation acc of epoch:
 90%|█████████ | 9/10 [10:49<01:04, 64.20s/it, best loss: -0.9873333328564962]                                                                              0.9853333333333333
 90%|█████████ | 9/10 [10:49<01:04, 64.20s/it, best loss: -0.9873333328564962]100%|██████████| 10/10 [10:49<00:00, 66.09s/it, best loss: -0.9873333328564962]
--- 662.8726398944855 seconds ---
Evalutation of best performing model:

   32/10000 [..............................] - ETA: 2s
  256/10000 [..............................] - ETA: 2s
  480/10000 [>.............................] - ETA: 2s
  704/10000 [=>............................] - ETA: 2s
  928/10000 [=>............................] - ETA: 2s
 1120/10000 [==>...........................] - ETA: 2s
 1312/10000 [==>...........................] - ETA: 2s
 1504/10000 [===>..........................] - ETA: 2s
 1696/10000 [====>.........................] - ETA: 2s
 1888/10000 [====>.........................] - ETA: 2s
 2080/10000 [=====>........................] - ETA: 2s
 2272/10000 [=====>........................] - ETA: 2s
 2464/10000 [======>.......................] - ETA: 1s
 2656/10000 [======>.......................] - ETA: 1s
 2848/10000 [=======>......................] - ETA: 1s
 3040/10000 [========>.....................] - ETA: 1s
 3232/10000 [========>.....................] - ETA: 1s
 3424/10000 [=========>....................] - ETA: 1s
 3616/10000 [=========>....................] - ETA: 1s
 3808/10000 [==========>...................] - ETA: 1s
 4000/10000 [===========>..................] - ETA: 1s
 4160/10000 [===========>..................] - ETA: 1s
 4320/10000 [===========>..................] - ETA: 1s
 4480/10000 [============>.................] - ETA: 1s
 4640/10000 [============>.................] - ETA: 1s
 4768/10000 [=============>................] - ETA: 1s
 4896/10000 [=============>................] - ETA: 1s
 5024/10000 [==============>...............] - ETA: 1s
 5152/10000 [==============>...............] - ETA: 1s
 5312/10000 [==============>...............] - ETA: 1s
 5472/10000 [===============>..............] - ETA: 1s
 5632/10000 [===============>..............] - ETA: 1s
 5792/10000 [================>.............] - ETA: 1s
 5984/10000 [================>.............] - ETA: 1s
 6176/10000 [=================>............] - ETA: 1s
 6368/10000 [==================>...........] - ETA: 1s
 6560/10000 [==================>...........] - ETA: 1s
 6752/10000 [===================>..........] - ETA: 0s
 6944/10000 [===================>..........] - ETA: 0s
 7136/10000 [====================>.........] - ETA: 0s
 7328/10000 [====================>.........] - ETA: 0s
 7520/10000 [=====================>........] - ETA: 0s
 7712/10000 [======================>.......] - ETA: 0s
 7904/10000 [======================>.......] - ETA: 0s
 8096/10000 [=======================>......] - ETA: 0s
 8288/10000 [=======================>......] - ETA: 0s
 8480/10000 [========================>.....] - ETA: 0s
 8672/10000 [=========================>....] - ETA: 0s
 8864/10000 [=========================>....] - ETA: 0s
 9056/10000 [==========================>...] - ETA: 0s
 9248/10000 [==========================>...] - ETA: 0s
 9440/10000 [===========================>..] - ETA: 0s
 9632/10000 [===========================>..] - ETA: 0s
 9824/10000 [============================>.] - ETA: 0s
10000/10000 [==============================] - 3s 296us/step
[0.047457141639397016, 0.9857]
Time consumed:  0.1850415133105384  hours
{'Dropout': 0.06874772122693129, 'Dropout_1': 0.293803364162271, 'Dropout_2': 1, 'Dropout_3': 0.11654484411565585, 'batch_size': 1, 'optimizer': 0}
