Leyendo  fashion
Executing  fashion with  Convolutional  arquitecture.

>>> Imports:
#coding=utf-8

try:
    from keras.datasets import mnist
except:
    pass

try:
    from keras.datasets import fashion_mnist
except:
    pass

try:
    from keras.datasets import cifar10
except:
    pass

try:
    from keras.datasets import imdb
except:
    pass

try:
    from keras import backend as K
except:
    pass

try:
    import numpy as np
except:
    pass

try:
    import sys
except:
    pass

try:
    import time
except:
    pass

try:
    import urllib.request
except:
    pass

try:
    import os
except:
    pass

try:
    from sklearn.model_selection import StratifiedKFold
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation, Flatten
except:
    pass

try:
    from keras.layers.convolutional import Convolution2D, MaxPooling2D
except:
    pass

try:
    from keras.optimizers import SGD, Adam
except:
    pass

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 0.5),
        'Dropout_1': hp.uniform('Dropout_1', 0, 0.5),
        'Dropout_2': hp.choice('Dropout_2', ['two','three']),
        'Dropout_3': hp.uniform('Dropout_3', 0, 0.5),
        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),
        'batch_size': hp.choice('batch_size', [64, 128]),
    }

>>> Data
  1: 
  2: (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
  3: 
  4: X_train = X_train.reshape(X_train.shape+(1,))
  5: X_test = X_test.reshape(X_test.shape+(1,))
  6: nb_classes = len(np.unique(y_train))
  7: 
  8: y_train = np_utils.to_categorical(y_train, nb_classes)
  9: y_test = np_utils.to_categorical(y_test, nb_classes)
 10: 
 11: 
 12: 
 13: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3: 	nb_classes = y_train.shape[1]
   4: 
   5: 	num_epoch=1*100
   6: 	num_epoch=1*1
   7: 
   8: 	model = Sequential()
   9: 
  10: 	model.add(Convolution2D(32, 3, 3, border_mode='same',
  11: 	                        input_shape=X_train.shape[1:]))
  12: 	model.add(Activation('relu'))
  13: 	model.add(Convolution2D(32, 3, 3))
  14: 	model.add(Activation('relu'))
  15: 	model.add(MaxPooling2D(pool_size=(2, 2)))
  16: 	model.add(Dropout(space['Dropout']))
  17: 
  18: 	model.add(Convolution2D(64, 3, 3, border_mode='same'))
  19: 	model.add(Activation('relu'))
  20: 	model.add(Convolution2D(64, 3, 3))
  21: 	model.add(Activation('relu'))
  22: 	model.add(MaxPooling2D(pool_size=(2, 2)))
  23: 	model.add(Dropout(space['Dropout_1']))
  24: 
  25: 	if space['Dropout_2'] == 'three':
  26: 		model.add(Convolution2D(128, 3, 3, border_mode='same'))
  27: 		model.add(Activation('relu'))
  28: 		model.add(Convolution2D(128, 3, 3))
  29: 		model.add(Activation('relu'))
  30: 		model.add(MaxPooling2D(pool_size=(2, 2)))
  31: 		model.add(Dropout(space['Dropout_3']))
  32: 
  33: 	model.add(Flatten())
  34: 	model.add(Dense(512))
  35: 	model.add(Activation('relu'))
  36: 	model.add(Dropout(0.5))
  37: 	model.add(Dense(nb_classes))
  38: 	model.add(Activation('softmax'))
  39: 
  40: 	 # let's train the model using SGD + momentum (how original).
  41: 	#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
  42: 	
  43: 	model.compile(loss='categorical_crossentropy',
  44: 	              optimizer=space['optimizer'],
  45: 	              metrics=['accuracy'])
  46: 
  47: 	result = model.fit(X_train, y_train,
  48: 	          batch_size=space['batch_size'],
  49: 	          nb_epoch=num_epoch,
  50: 	          verbose=2,
  51: 	          validation_split=0.1)
  52: 	validation_acc = np.amax(result.history['val_acc']) 
  53: 	print('Best validation acc of epoch:', validation_acc)
  54: 	return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}
  55: 
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz

 8192/29515 [=======>......................] - ETA: 0s
32768/29515 [=================================] - 0s 1us/step
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz

    8192/26421880 [..............................] - ETA: 6s
   49152/26421880 [..............................] - ETA: 51s
  122880/26421880 [..............................] - ETA: 31s
  253952/26421880 [..............................] - ETA: 20s
  409600/26421880 [..............................] - ETA: 17s
  434176/26421880 [..............................] - ETA: 19s
 1024000/26421880 [>.............................] - ETA: 9s 
 1089536/26421880 [>.............................] - ETA: 10s
 1228800/26421880 [>.............................] - ETA: 9s 
 1458176/26421880 [>.............................] - ETA: 9s
 1572864/26421880 [>.............................] - ETA: 9s
 1892352/26421880 [=>............................] - ETA: 8s
 2039808/26421880 [=>............................] - ETA: 8s
 2195456/26421880 [=>............................] - ETA: 8s
 2351104/26421880 [=>............................] - ETA: 8s
 2506752/26421880 [=>............................] - ETA: 8s
 2662400/26421880 [==>...........................] - ETA: 8s
 2826240/26421880 [==>...........................] - ETA: 8s
 2981888/26421880 [==>...........................] - ETA: 8s
 3121152/26421880 [==>...........................] - ETA: 8s
 3284992/26421880 [==>...........................] - ETA: 8s
 3457024/26421880 [==>...........................] - ETA: 7s
 3604480/26421880 [===>..........................] - ETA: 7s
 3776512/26421880 [===>..........................] - ETA: 7s
 3923968/26421880 [===>..........................] - ETA: 7s
 4104192/26421880 [===>..........................] - ETA: 7s
 4268032/26421880 [===>..........................] - ETA: 7s
 4423680/26421880 [====>.........................] - ETA: 7s
 4587520/26421880 [====>.........................] - ETA: 7s
 4759552/26421880 [====>.........................] - ETA: 7s
 4939776/26421880 [====>.........................] - ETA: 7s
 5111808/26421880 [====>.........................] - ETA: 7s
 5283840/26421880 [====>.........................] - ETA: 7s
 5455872/26421880 [=====>........................] - ETA: 6s
 5627904/26421880 [=====>........................] - ETA: 6s
 5799936/26421880 [=====>........................] - ETA: 6s
 5963776/26421880 [=====>........................] - ETA: 6s
 6135808/26421880 [=====>........................] - ETA: 6s
 6316032/26421880 [======>.......................] - ETA: 6s
 6496256/26421880 [======>.......................] - ETA: 6s
 6668288/26421880 [======>.......................] - ETA: 6s
 6840320/26421880 [======>.......................] - ETA: 6s
 7020544/26421880 [======>.......................] - ETA: 6s
 7200768/26421880 [=======>......................] - ETA: 6s
 7380992/26421880 [=======>......................] - ETA: 6s
 7561216/26421880 [=======>......................] - ETA: 6s
 7749632/26421880 [=======>......................] - ETA: 6s
 7929856/26421880 [========>.....................] - ETA: 5s
 8110080/26421880 [========>.....................] - ETA: 5s
 8282112/26421880 [========>.....................] - ETA: 5s
 8470528/26421880 [========>.....................] - ETA: 5s
 8658944/26421880 [========>.....................] - ETA: 5s
 8839168/26421880 [=========>....................] - ETA: 5s
 9027584/26421880 [=========>....................] - ETA: 5s
 9207808/26421880 [=========>....................] - ETA: 5s
 9396224/26421880 [=========>....................] - ETA: 5s
 9576448/26421880 [=========>....................] - ETA: 5s
 9764864/26421880 [==========>...................] - ETA: 5s
 9953280/26421880 [==========>...................] - ETA: 5s
10117120/26421880 [==========>...................] - ETA: 5s
10305536/26421880 [==========>...................] - ETA: 5s
10485760/26421880 [==========>...................] - ETA: 4s
10682368/26421880 [===========>..................] - ETA: 4s
10878976/26421880 [===========>..................] - ETA: 4s
11067392/26421880 [===========>..................] - ETA: 4s
11247616/26421880 [===========>..................] - ETA: 4s
11436032/26421880 [===========>..................] - ETA: 4s
11640832/26421880 [============>.................] - ETA: 4s
11845632/26421880 [============>.................] - ETA: 4s
12042240/26421880 [============>.................] - ETA: 4s
12238848/26421880 [============>.................] - ETA: 4s
12435456/26421880 [=============>................] - ETA: 4s
12632064/26421880 [=============>................] - ETA: 4s
12828672/26421880 [=============>................] - ETA: 4s
13025280/26421880 [=============>................] - ETA: 4s
13221888/26421880 [==============>...............] - ETA: 3s
13418496/26421880 [==============>...............] - ETA: 3s
13615104/26421880 [==============>...............] - ETA: 3s
13828096/26421880 [==============>...............] - ETA: 3s
14032896/26421880 [==============>...............] - ETA: 3s
14237696/26421880 [===============>..............] - ETA: 3s
14442496/26421880 [===============>..............] - ETA: 3s
14647296/26421880 [===============>..............] - ETA: 3s
14852096/26421880 [===============>..............] - ETA: 3s
15048704/26421880 [================>.............] - ETA: 3s
15253504/26421880 [================>.............] - ETA: 3s
15466496/26421880 [================>.............] - ETA: 3s
15679488/26421880 [================>.............] - ETA: 3s
15892480/26421880 [=================>............] - ETA: 3s
16105472/26421880 [=================>............] - ETA: 3s
16318464/26421880 [=================>............] - ETA: 2s
16531456/26421880 [=================>............] - ETA: 2s
16752640/26421880 [==================>...........] - ETA: 2s
16957440/26421880 [==================>...........] - ETA: 2s
17178624/26421880 [==================>...........] - ETA: 2s
17375232/26421880 [==================>...........] - ETA: 2s
17580032/26421880 [==================>...........] - ETA: 2s
17793024/26421880 [===================>..........] - ETA: 2s
17997824/26421880 [===================>..........] - ETA: 2s
18169856/26421880 [===================>..........] - ETA: 2s
18391040/26421880 [===================>..........] - ETA: 2s
18612224/26421880 [====================>.........] - ETA: 2s
18825216/26421880 [====================>.........] - ETA: 2s
19046400/26421880 [====================>.........] - ETA: 2s
19259392/26421880 [====================>.........] - ETA: 2s
19472384/26421880 [=====================>........] - ETA: 1s
19701760/26421880 [=====================>........] - ETA: 1s
19931136/26421880 [=====================>........] - ETA: 1s
20160512/26421880 [=====================>........] - ETA: 1s
20381696/26421880 [======================>.......] - ETA: 1s
20602880/26421880 [======================>.......] - ETA: 1s
20848640/26421880 [======================>.......] - ETA: 1s
21069824/26421880 [======================>.......] - ETA: 1s
21266432/26421880 [=======================>......] - ETA: 1s
21504000/26421880 [=======================>......] - ETA: 1s
21733376/26421880 [=======================>......] - ETA: 1s
21962752/26421880 [=======================>......] - ETA: 1s
22192128/26421880 [========================>.....] - ETA: 1s
22421504/26421880 [========================>.....] - ETA: 1s
22650880/26421880 [========================>.....] - ETA: 1s
22888448/26421880 [========================>.....] - ETA: 0s
23117824/26421880 [=========================>....] - ETA: 0s
23355392/26421880 [=========================>....] - ETA: 0s
23584768/26421880 [=========================>....] - ETA: 0s
23830528/26421880 [==========================>...] - ETA: 0s
24068096/26421880 [==========================>...] - ETA: 0s
24305664/26421880 [==========================>...] - ETA: 0s
24535040/26421880 [==========================>...] - ETA: 0s
24772608/26421880 [===========================>..] - ETA: 0s
25010176/26421880 [===========================>..] - ETA: 0s
25264128/26421880 [===========================>..] - ETA: 0s
25509888/26421880 [===========================>..] - ETA: 0s
25755648/26421880 [============================>.] - ETA: 0s
26001408/26421880 [============================>.] - ETA: 0s
26247168/26421880 [============================>.] - ETA: 0s
26427392/26421880 [==============================] - 7s 0us/step
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz

8192/5148 [===============================================] - 0s 0us/step
Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz

   8192/4422102 [..............................] - ETA: 1s
  49152/4422102 [..............................] - ETA: 7s
 114688/4422102 [..............................] - ETA: 5s
 278528/4422102 [>.............................] - ETA: 2s
 466944/4422102 [==>...........................] - ETA: 2s
 663552/4422102 [===>..........................] - ETA: 1s
 835584/4422102 [====>.........................] - ETA: 1s
1032192/4422102 [======>.......................] - ETA: 1s
1236992/4422102 [=======>......................] - ETA: 1s
1441792/4422102 [========>.....................] - ETA: 1s
1654784/4422102 [==========>...................] - ETA: 0s
1892352/4422102 [===========>..................] - ETA: 0s
2138112/4422102 [=============>................] - ETA: 0s
2408448/4422102 [===============>..............] - ETA: 0s
2695168/4422102 [=================>............] - ETA: 0s
3006464/4422102 [===================>..........] - ETA: 0s
3317760/4422102 [=====================>........] - ETA: 0s
3653632/4422102 [=======================>......] - ETA: 0s
4005888/4422102 [==========================>...] - ETA: 0s
4349952/4422102 [============================>.] - ETA: 0s
4423680/4422102 [==============================] - 1s 0us/step
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Train on 54000 samples, validate on 6000 samples
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                    Epoch 1/1
  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]                                                     - 48s - loss: 1.2191 - acc: 0.5645 - val_loss: 0.6296 - val_acc: 0.7687

  0%|          | 0/10 [00:48<?, ?it/s, best loss: ?]                                                    Best validation acc of epoch:
  0%|          | 0/10 [00:48<?, ?it/s, best loss: ?]                                                    0.7686666669845581
  0%|          | 0/10 [00:48<?, ?it/s, best loss: ?] 10%|█         | 1/10 [00:48<07:13, 48.19s/it, best loss: -0.7686666669845581]                                                                              Train on 54000 samples, validate on 6000 samples
 10%|█         | 1/10 [00:48<07:13, 48.19s/it, best loss: -0.7686666669845581]                                                                              Epoch 1/1
 10%|█         | 1/10 [00:48<07:13, 48.19s/it, best loss: -0.7686666669845581]                                                                               - 48s - loss: 1.3541 - acc: 0.7477 - val_loss: 0.4471 - val_acc: 0.8512

 10%|█         | 1/10 [01:36<07:13, 48.19s/it, best loss: -0.7686666669845581]                                                                              Best validation acc of epoch:
 10%|█         | 1/10 [01:36<07:13, 48.19s/it, best loss: -0.7686666669845581]                                                                              0.8511666666666666
 10%|█         | 1/10 [01:36<07:13, 48.19s/it, best loss: -0.7686666669845581] 20%|██        | 2/10 [01:36<06:26, 48.25s/it, best loss: -0.8511666666666666]                                                                              Train on 54000 samples, validate on 6000 samples
 20%|██        | 2/10 [01:37<06:26, 48.25s/it, best loss: -0.8511666666666666]                                                                              Epoch 1/1
 20%|██        | 2/10 [01:37<06:26, 48.25s/it, best loss: -0.8511666666666666]                                                                               - 50s - loss: 0.6562 - acc: 0.7633 - val_loss: 0.3651 - val_acc: 0.8647

 20%|██        | 2/10 [02:26<06:26, 48.25s/it, best loss: -0.8511666666666666]                                                                              Best validation acc of epoch:
 20%|██        | 2/10 [02:26<06:26, 48.25s/it, best loss: -0.8511666666666666]                                                                              0.8646666668256124
 20%|██        | 2/10 [02:26<06:26, 48.25s/it, best loss: -0.8511666666666666] 30%|███       | 3/10 [02:26<05:41, 48.82s/it, best loss: -0.8646666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 30%|███       | 3/10 [02:27<05:41, 48.82s/it, best loss: -0.8646666668256124]                                                                              Epoch 1/1
 30%|███       | 3/10 [02:27<05:41, 48.82s/it, best loss: -0.8646666668256124]                                                                               - 46s - loss: 2.6867 - acc: 0.6344 - val_loss: 0.4785 - val_acc: 0.8385

 30%|███       | 3/10 [03:13<05:41, 48.82s/it, best loss: -0.8646666668256124]                                                                              Best validation acc of epoch:
 30%|███       | 3/10 [03:13<05:41, 48.82s/it, best loss: -0.8646666668256124]                                                                              0.8385
 30%|███       | 3/10 [03:13<05:41, 48.82s/it, best loss: -0.8646666668256124] 40%|████      | 4/10 [03:13<04:49, 48.22s/it, best loss: -0.8646666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 40%|████      | 4/10 [03:14<04:49, 48.22s/it, best loss: -0.8646666668256124]                                                                              Epoch 1/1
 40%|████      | 4/10 [03:14<04:49, 48.22s/it, best loss: -0.8646666668256124]                                                                               - 48s - loss: 0.7201 - acc: 0.7480 - val_loss: 0.4347 - val_acc: 0.8285

 40%|████      | 4/10 [04:02<04:49, 48.22s/it, best loss: -0.8646666668256124]                                                                              Best validation acc of epoch:
 40%|████      | 4/10 [04:02<04:49, 48.22s/it, best loss: -0.8646666668256124]                                                                              0.8285000004768371
 40%|████      | 4/10 [04:02<04:49, 48.22s/it, best loss: -0.8646666668256124] 50%|█████     | 5/10 [04:02<04:02, 48.40s/it, best loss: -0.8646666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 50%|█████     | 5/10 [04:02<04:02, 48.40s/it, best loss: -0.8646666668256124]                                                                              Epoch 1/1
 50%|█████     | 5/10 [04:02<04:02, 48.40s/it, best loss: -0.8646666668256124]                                                                               - 45s - loss: 14.4733 - acc: 0.1016 - val_loss: 14.5305 - val_acc: 0.0985

 50%|█████     | 5/10 [04:47<04:02, 48.40s/it, best loss: -0.8646666668256124]                                                                              Best validation acc of epoch:
 50%|█████     | 5/10 [04:47<04:02, 48.40s/it, best loss: -0.8646666668256124]                                                                              0.09849999998013179
 50%|█████     | 5/10 [04:47<04:02, 48.40s/it, best loss: -0.8646666668256124] 60%|██████    | 6/10 [04:47<03:09, 47.42s/it, best loss: -0.8646666668256124]                                                                              Train on 54000 samples, validate on 6000 samples
 60%|██████    | 6/10 [04:47<03:09, 47.42s/it, best loss: -0.8646666668256124]                                                                              Epoch 1/1
 60%|██████    | 6/10 [04:47<03:09, 47.42s/it, best loss: -0.8646666668256124]                                                                               - 46s - loss: 0.7089 - acc: 0.7974 - val_loss: 0.3707 - val_acc: 0.8662

 60%|██████    | 6/10 [05:33<03:09, 47.42s/it, best loss: -0.8646666668256124]                                                                              Best validation acc of epoch:
 60%|██████    | 6/10 [05:33<03:09, 47.42s/it, best loss: -0.8646666668256124]                                                                              0.8661666663487753
 60%|██████    | 6/10 [05:33<03:09, 47.42s/it, best loss: -0.8646666668256124] 70%|███████   | 7/10 [05:33<02:20, 46.99s/it, best loss: -0.8661666663487753]                                                                              Train on 54000 samples, validate on 6000 samples
 70%|███████   | 7/10 [05:33<02:20, 46.99s/it, best loss: -0.8661666663487753]                                                                              Epoch 1/1
 70%|███████   | 7/10 [05:33<02:20, 46.99s/it, best loss: -0.8661666663487753]                                                                               - 45s - loss: 0.8660 - acc: 0.7266 - val_loss: 0.4852 - val_acc: 0.8338

 70%|███████   | 7/10 [06:18<02:20, 46.99s/it, best loss: -0.8661666663487753]                                                                              Best validation acc of epoch:
 70%|███████   | 7/10 [06:18<02:20, 46.99s/it, best loss: -0.8661666663487753]                                                                              0.8338333330154419
 70%|███████   | 7/10 [06:18<02:20, 46.99s/it, best loss: -0.8661666663487753] 80%|████████  | 8/10 [06:18<01:32, 46.43s/it, best loss: -0.8661666663487753]                                                                              Train on 54000 samples, validate on 6000 samples
 80%|████████  | 8/10 [06:19<01:32, 46.43s/it, best loss: -0.8661666663487753]                                                                              Epoch 1/1
 80%|████████  | 8/10 [06:19<01:32, 46.43s/it, best loss: -0.8661666663487753]                                                                               - 48s - loss: 1.8907 - acc: 0.6655 - val_loss: 0.4526 - val_acc: 0.8428

 80%|████████  | 8/10 [07:07<01:32, 46.43s/it, best loss: -0.8661666663487753]                                                                              Best validation acc of epoch:
 80%|████████  | 8/10 [07:07<01:32, 46.43s/it, best loss: -0.8661666663487753]                                                                              0.8428333336512248
 80%|████████  | 8/10 [07:07<01:32, 46.43s/it, best loss: -0.8661666663487753] 90%|█████████ | 9/10 [07:07<00:47, 47.15s/it, best loss: -0.8661666663487753]                                                                              Train on 54000 samples, validate on 6000 samples
 90%|█████████ | 9/10 [07:08<00:47, 47.15s/it, best loss: -0.8661666663487753]                                                                              Epoch 1/1
 90%|█████████ | 9/10 [07:08<00:47, 47.15s/it, best loss: -0.8661666663487753]                                                                               - 50s - loss: 0.6478 - acc: 0.7682 - val_loss: 0.3680 - val_acc: 0.8690

 90%|█████████ | 9/10 [07:58<00:47, 47.15s/it, best loss: -0.8661666663487753]                                                                              Best validation acc of epoch:
 90%|█████████ | 9/10 [07:58<00:47, 47.15s/it, best loss: -0.8661666663487753]                                                                              0.869
 90%|█████████ | 9/10 [07:58<00:47, 47.15s/it, best loss: -0.8661666663487753]100%|██████████| 10/10 [07:58<00:00, 48.20s/it, best loss: -0.869]            
--- 487.2326521873474 seconds ---
Evalutation of best performing model:

   32/10000 [..............................] - ETA: 1s
  320/10000 [..............................] - ETA: 1s
  608/10000 [>.............................] - ETA: 1s
  896/10000 [=>............................] - ETA: 1s
 1184/10000 [==>...........................] - ETA: 1s
 1472/10000 [===>..........................] - ETA: 1s
 1760/10000 [====>.........................] - ETA: 1s
 2048/10000 [=====>........................] - ETA: 1s
 2336/10000 [======>.......................] - ETA: 1s
 2624/10000 [======>.......................] - ETA: 1s
 2912/10000 [=======>......................] - ETA: 1s
 3200/10000 [========>.....................] - ETA: 1s
 3488/10000 [=========>....................] - ETA: 1s
 3744/10000 [==========>...................] - ETA: 1s
 4032/10000 [===========>..................] - ETA: 1s
 4320/10000 [===========>..................] - ETA: 1s
 4608/10000 [============>.................] - ETA: 0s
 4896/10000 [=============>................] - ETA: 0s
 5184/10000 [==============>...............] - ETA: 0s
 5472/10000 [===============>..............] - ETA: 0s
 5760/10000 [================>.............] - ETA: 0s
 6048/10000 [=================>............] - ETA: 0s
 6336/10000 [==================>...........] - ETA: 0s
 6624/10000 [==================>...........] - ETA: 0s
 6912/10000 [===================>..........] - ETA: 0s
 7200/10000 [====================>.........] - ETA: 0s
 7488/10000 [=====================>........] - ETA: 0s
 7776/10000 [======================>.......] - ETA: 0s
 8064/10000 [=======================>......] - ETA: 0s
 8352/10000 [========================>.....] - ETA: 0s
 8608/10000 [========================>.....] - ETA: 0s
 8864/10000 [=========================>....] - ETA: 0s
 9120/10000 [==========================>...] - ETA: 0s
 9312/10000 [==========================>...] - ETA: 0s
 9504/10000 [===========================>..] - ETA: 0s
 9728/10000 [============================>.] - ETA: 0s
 9984/10000 [============================>.] - ETA: 0s
10000/10000 [==============================] - 2s 191us/step
[0.38814576473236084, 0.8584]
Time consumed:  0.1359547355439928  hours
{'Dropout': 0.37231452533627063, 'Dropout_1': 0.06045109560475492, 'Dropout_2': 1, 'Dropout_3': 0.10471119809697471, 'batch_size': 0, 'optimizer': 1}
